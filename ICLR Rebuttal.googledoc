ICLR Rebuttal:

TODO Klemen:
Train WavCoch on LibriSpeech 960
Make Diagram of WavCoch + CochStream architecture
Write response template
Write mathy description of cochleagram
Train WavCoch ablations on LibriSpeech with varying codebook size
Train CochStream Model on LibriSpeech + LibriLigh
Train Vocode on new Cochlear tokens
Evaluate Vocoder on Cochlear tokens
Optimize SUPERB hyperparameters
Run SUPERB evals on new models

Individual response to three reviewers
Thanks again for the time spent reviewing our work. We have now addressed all of your questions and concerns through the individual response (posted last week) as well as through the global response. If you have any further questions that are preventing you from raising our score, please let us know and we will do our best to address them in the extended discussion period.

Individual response to Nhso
We thank the reviewer for the valuable feedback, and for engaging in the discussion. We provide some additional explanations below:

Probing an intermediate representation as discrete tokens

We thank the reviewer for providing relevant references for our WavCoch quantizer. These have been included in the revised discussion. We would like to point out a few key distinctions with the two references: 
CLaM-TTS does indeed auto-encode a mel-spectrogram as the reviewer mentions. In their Mel-VAE pipeline, the mel conversion is performed as a pre-processing step on the input audio, and thus the actual Mel-VAE objective is an autoencoder with the same target as the input to the neural network. 
Similarly, Spectral Codec computes the mel-spectrogram of the input audio, and uses HiFi-GAN as their decoder, which also utilizes the mel-spectrogram as the prediction objective. In contrast, we utilize only the cochleagram as our target objective, allowing the model to learn how to perform the waveform-to-cochleagram transform by itself. The reasoning behind this approach is that the process of learning to transform the sound into the spectral domain itself might yield useful representations of the underlying audio. Taken together, we still believe that WavCoch is novel in its way of learning to transform one representation into another representation through a discrete quantization bottleneck. 


Mel-spectrogram vs. cochleagram

In order to investigate this important question we trained a version of WavCoch using mel-spectrograms as targets as discussed in our global response. We discovered that the codes have relatively similar vocabulary usage and similar phoneme cluster purity to that learned from cochleagram representation. If anything, the cochleagram representation was slightly superior to mel-spectrograms, but by a small margin. Furthermore, the strong biological evidence for utilizing the cochleagram backs our decision with respect to producing a more biologically-plausible speech model than competing approaches.
Finally, in the revised paper, we have added a discussion on the mel-spectrogram versus cochleagram issue and explicitly note that we do not claim the superiority of cochleagrams over mel-spectrograms.

If you have any further questions that are preventing you from raising our score, please let us know and we will do our best to address them in the extended discussion period.

Global Responses:

We once again thank the reviewers for their constructive and thoughtful feedback which has significantly improved the paper over the rebuttal period. We begin by outlining the key strengths of the paper as identified by the reviewers, followed by a list of the identified suggestions/concerns and the steps we have taken to address them.

Strengths
Interesting and novel biologically-inspired speech representation learning framework (mentioned by ipew, npk7, 6c7S, Nhso. For example, Nhso noted: “The paper takes an interesting approach by introducing a cochlear-inspired representation (cochleagram) for speech modeling, which is relatively uncommon in the field”)

Improved interpretability of speech representations (mentioned by ipew, npk7, Nhso. For example, npk7 noted: “The model can visualize predictions in cochleagram form, offering interpretable insights into its speech representations”)

Competitive and versatile task performance (mentioned by ipew, npk7, 6c7S, Nhso. For example, ipew noted: “The authors validate the model’s versatility across various tasks (phoneme/word decoding, lexical semantics) and benchmarks (e.g., SUPERB), demonstrating its competitive performance and interpretability advantages over existing models”)

Clear writing and good high-level motivation (mentioned by 6c7S, Nhso. For example, 6c7S noted: “The paper is well-written, with a clear and interesting motivation rooted in mimicking the human auditory system”)  


Weaknesses

Open-source data reproduction

We thank reviewer Nhso for their suggestion, and in line with our strong commitment to open science, we have reproduced key results using a model trained on fully openly available datasets. First, we retrained a version of WavCoch using the librispeech960 dataset consisting of 960 hours of read English speech [1]. Second, we train a 1B parameter version of the sequence-to-sequence CochStream model on the full 60k hours of the libri-light dataset consisting of 60,000 hours of unlabeled English speech [2]. As demonstrated here, we show that this model demonstrates strong performance on a similar set of tasks than the version trained on our dataset (see Section below titled “CochStream Libri-Light task performance analysis”). This model serves as a strong replication of the results from our initial model using popular open-source datasets. 


WavCoch vocabulary size analysis

As suggested by reviewers Nhso and npk7, we performed ablations on the vocabulary size of the WavCoch model. In an attempt to ensure maximal reproducibility, we perform these ablations using the librispeech960 dataset as mentioned above. We train variants of WavCoch using a vocabulary size of 16384, 8192 and 4096 (14, 13 and 12 bit codes respectively). We report the results in Figure 3 (https://anonymous.4open.science/w/cochstream-project-page-0546/) which shows the phoneme cluster purity and reconstruction error on an out of distribution test set (TIMIT test set). We introduced the cluster purity metric in our original response (LINK) which is defined as purity = (Count of most associated phoneme for token i) / (total counts for token i) which intuitively provides a metric for how consistently a given token aligns with a specific phoneme.

The plot shows that a vocabulary size of 8192 presents both the highest cluster purity and lowest reconstruction error. Furthermore, this vocabulary size presents a local minima in the MSE space, and a maxima in the cluster purity space indicating it is the optimal size for generalization in this domain. We are very grateful to the reviewers’ suggestion as it revealed an improvement in our model design. We will use a vocabulary of 8192 for all future models. 




Vocabulary token clustering analysis

As suggested by reviewers Nhso and npk7, we analyzed the distribution of tokens associated with specific phonemes in the TIMIT test set. We visualized the distribution in Figure 4 ( https://anonymous.4open.science/w/cochstream-project-page-0546/)  and found that multiple tokens are typically associated with each phoneme. Notably, a large number of tokens are linked to the "sil" (silence) label, but we would like to clarify that this label includes any non-speech sound, not true silence. Since WavCoch is optimized with a single objective–to predict the cochleagram as best as possible–it has no direct incentive to collapse the codebook by merging codes associated with the same phoneme. However, our analyses show that this clustering takes place naturally, to an extent. In future work, we plan to investigate WavCoch models that explicitly incorporate a more efficient codebook, and compare it against the current baseline. 

Mel spectrogram vs. cochleagram

As raised by reviewers ipew and Nhso, we trained a version of our WavCoch model using a standard 80 mel-spectrogram representation (instead of a cochleagram representation). We trained it on the publicly available librispeech960 dataset, consisting of 960 hours of speech recordings [1]. Since the spectrogram L2 reconstruction error is not directly comparable between a cochleagram and mel-spectrogram we utilize two proxy measures: i) Number of unique codes utilized and ii) Phoneme cluster purity. Both of these metrics are computed on the out-of-distribution TIMIT test set. 
First, related to the number of unique codes utilized: We find that the WavCoch model trained with mel spectrograms utilized 8151 out of 8192 codes, while the cochleagram version utilized 8172 codes. Second, related to phoneme cluster purity: We find that the mel spectrogram model achieved an average phoneme cluster purity of 0.3473 while the model trained with the cochleagram achieved an average phoneme cluster purity of 0.3517. While these results are preliminary, they suggest that the cochleagram representation performs at least as well as, if not slightly better than, the mel spectrogram in this context. We are continuously exploring this comparison further, and will report the findings in the final version of the manuscript.
Finally, besides the quantitative analyses reported above, we prefer the cochleagram over the mel-spectrogram representation for conceptual reasons: The ultimate goal of our framework is to move towards more biologically plausible speech models, and the cochleagram is more aligned with this goal. 

In the revised paper, we have added a discussion on the mel-spectrogram versus cochleagram issue and explicitly note that we do not claim the superiority of cochleagrams over mel-spectrograms.

CochStream Libri-Light task performance analysis

As raised by reviewer Nhso, we trained a version of CochStream using the 60k hour Libri-Light dataset [2] (denoted as CochStream-large-ll in the updated version of the paper). We utilized cochlear tokens produced by the new WavCoch model with a vocabulary size of 8192 which proved superior in the experiments mentioned above. We also adjusted the CochStream Transformer parameters to exactly match those of HuBERT (and WavLM) for an even more apples-to-apples comparison (suggested by e.g., reviewer ipew). While our full training schedule is set for 500k optimization steps, we were not able to complete it in time for rebuttal due to time and computational constraints. Nevertheless, even at 200k steps our model already matches the performance of the model we described in the initial submission. We highlight some of the results below:

On the TIMIT linear probing task the libri-light model matches the performance of the one trained on our data on phoneme decoding (92% old vs. 92% new) while it outperforms it on the word decoding task (67% old vs. 69% new).
On the ZeroSpeech 2021 Lexical Semantic Benchmark the libri-light model scored above the old base model and all other baselines we evaluated, but below the large model trained on our dataset. This suggests that for this task, both our modeling approach and our dataset are important for maximizing the performance.
On the SUPERB benchmark the libri-light model faired comparably to the model trained on our data.

For the full results please see tables 1, 2 and 3 in the updated main paper pd.

We believe that these results demonstrate that the core performance contribution of our work comes from our novel two-stage modeling framework (WavCoch tokenization followed by CochStream auto-regressive prediction). Furthermore, this demonstrates that our model is not contingent on one single dataset – we obtain very competitive performance on different audio datasets. We are continuously evaluating our models on SUPERB, and will send a further update in the extended discussion period about performance characteristics as suggested by reviewers.


Interpretability of speech continuation capability 

As suggested by reviewers npk7 and 6c7S, we investigate the decoding of CochStream predictions back into the auditory signal. To this end, we developed a simple procedure for inverting the cochleagrams back into a waveform. This procedure is a per-sample optimization of making the waveform match the cochleagram prediction. 

Specifically, we optimize a 1D tensor representing the waveform input to make its cochleagram representation match the cochleagram predicted by WavCoch (via L2 error). We backpropagate through the cochleagram transformation by using the Adam optimizer with a learning rate of 1e-2.
Note that this optimization procedure is not a learned vocoder model, but a simple procedure of converting the output of WavCoch, the cochleagrams, into audible sound (conceptually similar to Griffin-Lim algorithm [3]).

We upload several audible samples of speech generations from our CochStream: (TODO: Link).

We observe that on short time-scales the model produces reasonable completions, but the longer the completion, the more the predictions drift away from being plausible. We would like to emphasize that the purpose of CochStream is not to be a language model, but a speech representation model – the fact that it can perform rudimentary language modeling is a serendipitous side effect of the training objective, which points to the fact that understanding speech, and producing language can be thought of as a unified objective. These findings serve as great motivating factors for a planned follow-up work which will attempt to stabilize longer term speech generations, building on top of the foundation laid out in this paper.


[1] Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015, April). Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP) (pp. 5206-5210). IEEE.

[2] Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazaré, P. E., ... & Dupoux, E. (2020, May). Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7669-7673). IEEE.

[3] Griffin D. and Lim J. (1984). "Signal Estimation from Modified Short-Time Fourier Transform". IEEE Transactions on Acoustics, Speech and Signal Processing. 32 (2): 236–243. doi:10.1109/TASSP.1984.1164317



Initial Responses:
R1 Ipew

We thank the reviewer for their careful review and thoughtful feedback! Below, we address all the reviewer’s comments, except for the comparison to additional baseline models and a more thorough analysis of model performance, which we are currently working on. We are happy to further clarify any of the answers below.


Weakness 1. The paper lacks clear explanations of the cochlear encoding process, its relation to mel spectrograms, and model diagrams. 

We have created a detailed model diagram, demonstrating the WavCoch Quantizer architecture as well as the CochStream sequence-to-sequence architecture, please see the figure uploaded at this anonymous link.

We will include the model diagram in the revised manuscript along with a detailed description:

WavCoch Quantizer Architecture: First, the raw waveform (shape: [1,80000] for 5s of mono audio sampled at 16kHz) undergoes the Fourier Transform by computing Twiddle Factors \cite{Cooley1965AnAF}. These factors represent complex sinusoidal components that decompose the signal into its frequency spectrum. The Twiddle Factors are applied to the audio signal through a 1D convolution (window size 1,001 and hop length 80 samples) which transforms the signal into the time-frequency domain. 
Second, each 5 ms temporal step of this time-frequency representation is fed into two fully-connected (FC) layers with ReLU nonlinearities (with 512 hidden units each).
Third, these embeddings are then passed through a 14-dimensional LFQ bottleneck \cite{MagViT2}, which effectively binarizes the representation. We read out the activations of this bottleneck as a 14-bit binary code which can be interpreted as one of 2^14 = 16,384 discrete tokens.
Fourth, the output of the LFQ bottleneck is then projected to a 211 dimensional output, through two 1-dimensional convolutional layers (kernel size 10 and stride 1), separated by ReLU nonlinearities. This output corresponds to the frequencies in the cochleagram representation \cite{feather2023model} which it is supervised to match via L2 error.
Thus for every 5 seconds of audio, WavCoch extracts a sequence of 988 integers in the range [0, 16384) through the LFQ bottleneck, denoted as \textbf{cochlear tokens}, to feed into CochStream.

CochStream Autoregressive Model Architecture: The cochlear tokens obtained in WavCoch are passed to a GPT-style autoregressive Transformer \cite{radford_improving_2018}, denoted as CochStream. We train two versions: CochStream-base (97M parameters), with 12 layers, 12 attention heads and an embedding size of 784 and CochStream-large (1.3B parameters) with 24 layers, 16 attention heads, and an embedding size of 2,048. Both models have a vocabulary size of 16,384. The CochStream model takes as input the cochlear token sequence produced by WavCoch (Section \ref{methods:wavcoch}) and predicts the next token in the sequence. The context length is approximately 20s (4,096 tokens). We utilize a learned positional embedding and compute the cross-entropy loss between the predicted logits and the true next token in the sequence.

We are very happy to supply the actual implementation to the reviewers if they wish, and will publicly release the full code base upon acceptance.

Finally, we clarify WavCoch’s relation to a mel spectrogram: WavCoch could operate with a “simple” mel spectrogram as the target instead of a cochleagram. We do not make any claims about the superiority of a cochleagram over a mel spectrogram (we have clarified this in the revised manuscript). Instead, the main novelty from WavCoch lies in encoding one representation (in this case, the waveform) into another representation known to be computed in the auditory processing hierarchy (cochleagram) and “probing” an intermediary representation as discrete tokens. We chose the cochleagram for its biological plausibility.

The cochleagram implementation that we use can be found online in the following public repository, which we will link to in our revised manuscript along with a brief description: https://github.com/jenellefeather/chcochleagram.


Weakness 2. Issue: Linear probing performance lacks relevant models.

We agree that smaller models would be helpful. We are currently working on including the models the reviewer suggested (WavLM-base or wav2vec2-base) for comparison the in linear probing results. Following these results, we will also provide a more thorough analysis of our model’s performance by the end of the rebuttal period. 

Questions 1 and 2. Issue: Lacking information about CochStream embeddings.

We apologize for the oversight. Here is a description, which we have of course included in the revised manuscript:

We obtain CochStream embeddings by pooling the embeddings of all the tokens associated with the corresponding temporal section of the cochleagram via ground-truth phoneme or word boundaries (Section \ref{results:repr}). For the pooling operation, we tested mean/max/min pooling for the linear probing experiments and lexical similarity for CochStream and the comparison models.

Finally, to answer the reviewer’s question related to how obtaining embeddings from GPT-style models like CochStream relates to masked models like HuBERT: 
When analyzing a sound (such as when we obtain embeddings for a given phoneme/word), all computations are performed in parallel (using a causal mask, like during training) eliminating any inference time bottlenecks caused by the causality constraint. 
We note that our model is of course also capable of generating long-form continuations of audio, which does take longer to rollout (much like an LLM) – however this is a capability that is not present in competitor models such as HuBERT leaving no clear point of comparison.


R2 npk7

We thank the reviewer for their positive feedback and insightful questions. Below we XXX.

Weakness 1. Issue: Baseline models seem weak.

We will include an audio autoencoding (neural codec) model baseline as the reviewer suggested. We believe that Whisper is not a fair comparison given that it is trained on labeled text data. Instead, we believe that other self-supervised models are fair comparisons. 

Weakness 2. Issue: Main advantages of CochStream.

We are working on including additional comparison models per the reviewers’ comments, and following these results, we will provide a more thorough analysis of our model’s advantages by the end of the rebuttal period.  

Weakness 3. Issue: Vocabulary utilization rate of WavCoch.

We thank the reviewer for raising this point. We would like to clarify that the cochlear tokens are meant to be a lightweight encoding of the underlying auditory stream (much like BPE tokens for text), not a deep semantic clustering of features. Nevertheless, we conducted some cluster purity experiments and surprisingly found that even at the token level the model starts to extract structures that are loosely related to phonemes. 

We compiled some initial results into a plot which can be found at the bottom of our anonymous website (https://anonymous.4open.science/w/cochstream-project-page-0546/) as Figure 2. First of all, we asked how many tokens were utilized for the out of distribution TIMIT test set. We found that 82.5% tokens were utilized (13,517 tokens out of 16,384).
Second, we investigated the vocabulary utilization rate in relation to the phoneme purity of each token. For each token, we define the purity as:

Purity = (Count of most associated phoneme for token i) / (total counts for token i) 

Hence, the purity (on Figure 2’s y-axis) shows the purity for each token, whereas the color shows how many times a given token appears in the test set. As evident from the plot, even tokens that occur more than 70 times each have high purity values, up to 0.85.



Inspired by these initial results we will conduct a more thorough investigation of the cochlear token space, with varying bottleneck sizes during the course of the rebuttal period. We will include a detailed section on these results in the revised manuscript.

Weakness 4. Issue: Long continuations based on a seed.

It is correctly observed that CochStream generates relatively long waveform continuations that appear realistic. To perform some preliminary analysis of their quality we are training a vocoder model to decode the continuations back to audio. We will show some preliminary results by the end of the rebuttal period and will include a deeper analysis in the final manuscript. 
We also want to emphasize that the focus of the current paper is speech representation learning, and not language modeling and hence we leave the longer continuations and linguistic benchmarks for a separate paper.

Weakness 5. Issue: Autoregressive modeling does not capture the complexity of biological systems.

We agree with the reviewer’s point: it is likely that an algorithm that captures the full complexity of biological systems might need to include different kinds of recurrence, wiring constraints, different cell types etc. – that said, very little is known about the neural architecture and learning rules underlying auditory processing in humans, and hence we believe that we cannot a priori reject autoregressive learning. Below, we emphasize three points as to why autoregressive Transformers might serve as a useful approximation in the current context:

i) It is well-established that prediction is a neural objective [1] and hence an autoregressive Transformer is a reasonable candidate hypothesis for prediction-based processing in auditory processing at an algorithmic level, but likely not at the implementation-level [2]. 
ii) Transformers are incredibly powerful for many biologically-plausible tasks, and we wanted to build a model capable of handling various audio-based tasks that humans perform. In the perfect world, we would love to develop a model that perfectly aligns with neural circuitry at a fine-grained implementation-level and is powerful for tasks, but given that we do not know precisely the implementation-level details of auditory processing yet, we think the Transformer serves as a powerful stand-in. 
iii) A large transformer with an autoregressive training objective can be viewed as evolving the  circuitry that a biological brain is born with. Computational models did not undergo evolution in the same way as humans (or other species) did – hence, a large training phase where a generic model architecture is exposed to statistical regularities of the world (in this case, speech) may serve to wire up function-specific architecture similar to what a brain is born with. 


Finally, as we emphasize in the discussion of the paper, “We by no means claim that CochStream is a perfect biologically-inspired model, but it is a critical step in the right direction.”. We believe our paper is a big step in the right direction through i) more biologically plausible input representations compared to other quantization schemes, ii) no global clustering procedures, iii) no large intra-batch comparisons, and iv) no bidirectionality. In the discussion, we additionally refer to several works that argue that Transformer computations can be implemented in biological tissue [3-5].


References
[1] Spratling, Michael W. "A review of predictive coding algorithms." Brain and cognition 112 (2017): 92-97.
[2] Marr, David, and Tomaso Poggio. "From understanding computation to understanding neural circuitry." (1976).
[3] Bricken, T., & Pehlevan, C. (2021). Attention approximates sparse distributed memory. Advances in Neural Information Processing Systems, 34, 15301-15315.
[4] Whittington, James CR, Joseph Warren, and Timothy EJ Behrens. "Relating transformers to models and neural representations of the hippocampal formation." arXiv preprint arXiv:2112.04035 (2021).
[5] Kozachkov, Leo, Ksenia V. Kastanenka, and Dmitry Krotov. "Building transformers from neurons and astrocytes." Proceedings of the National Academy of Sciences 120.34 (2023): e2219150120.



R3 6c7S

We are very encouraged by the reviewer’s positive feedback and acknowledgement of the impact of the work, and are thankful for their thoughtful comments.

Weakness. Reliance on ML benchmarks. 

First, we appreciate the reviewer highlighting the importance of biologically-relevant benchmarking, which we agree demonstrates a significant gap in the literature. While ML benchmarks are of course standard for evaluating audio-based models, the lack of established biologically-relevant benchmarks becomes evident as (a subset of) the field moves toward more human-inspired models. Through this work, and in planned follow-up studies, we aim to contribute to closing this gap. 
We want to emphasize that we included the lexical similarity benchmark (sSIMI), which is not a fully standard ML benchmark as it involves direct comparison to human performance. Hence, this represents an initial step toward incorporating more human-based evaluations. 

Second, as the reviewer mentions, we believe that a fundamental advantage of CochStream is that it can naturally be prompted with sounds (much like a language model can be prompted with text). This capability can indeed pave the way for a new era of cognitively inspired prompt-benchmarking of auditory models. Since our model is unique in this capability we were unable to compare it to any baselines, however we will attempt to perform a pilot evaluation of the continuations by training a vocoder to obtain audible continuations (also see response to Question 2 below) during the rebuttal period. We will include a detailed discussion of this paradigm and its implications in the final manuscript.

Third, if the reviewer has a particular biologically-inspired evaluation in mind for the current paper, we would be happy to test our model on it (depending on the benchmark’s scope, of course). Otherwise, we are very interested in pursuing this direction in future papers, once we have introduced the model in the current paper. 




OTHER BIO PLAUSIBLE EVALS: https://arxiv.org/abs/2407.03005




Question 1. Performance on non-content tasks.
We will provide a more thorough analysis of our model’s performance characteristics and unique advantages by the end of the rebuttal period, including addressing the disparity between the performance on different types of tasks.

Question 2. Speech continuation capability.
To perform some preliminary analysis of our models’s speech continuation capability, we are currently training a vocoder model to decode the continuations back to audio. We will show some preliminary results by the end of the rebuttal period and will include a deeper analysis, including qualitative human evaluations, in the final manuscript. 
We also want to emphasize that the focus of the current paper is speech representation learning, and not language modeling and hence we leave the longer continuations and linguistic benchmarks for a separate paper.


Question 3. Motivation for autoregressive training.

We chose the autoregressive training objective for three main reasons (which we will include in the discussion of the revised manuscript):

i) Amongst the algorithms commonly used in modern AI, we believe autoregressive learning is the most biologically plausible one. Unlike the BERT objective, it does not violate causality, and does not require us to discover units through global k-means clustering for subsequent use in autoregressive sequence models. Unlike contrastive objectives it does not require a large selection of heuristically-picked positive and negative examples. Instead it simply predicts the distribution of next plausible sounds. 

ii) Autoregressive training is simple, yet capable of capturing enormous complexity when appropriately scaled. In the paper, we demonstrate that the autoregressive objective scales well within the speech domain (comparison of CochStream-base versus CochStream-large).

iii) Autoregressive training allows for the evaluation of the surprisal associated with any given sequence of sounds, and for the speech continuation capability discussed in Question 2 above.






R4 Nhso

We appreciate the reviewer’s acknowledgement of the originality, quality, and significance of the work. We thank the reviewer for their careful review. We provide clarifications to the reviewer’s comments/questions, and are currently implementing analyses suggested by the reviewer (training models on open-source data; verifying the efficiency of cochlear tokens), as clarified below.

Weakness 1. LFQ and Transformer lack biological basis.
We fully agree that not every part of our pipeline is supported by biological evidence, however, we also emphasize that the precise architecture underlying human auditory processing is not known, and hence we believe that LFQ and Transformers can serve as powerful stand-ins based on the heuristics detailed below.

LFQ: While LFQ itself might not be biologically inspired, we believe that our use of it in WavCoch has a biologically sensible interpretation: WavCoch implements a biological transformation (waveform to cochlear representation) and we contend that the LFQ bottleneck can be viewed as a readout of the discrete firing patterns of the neurons along this (largely unknown) pathway. The representations obtained from LFQ are binary 14-bit vectors which can be interpreted as an on-off neural firing pattern [1] which are then processed by downstream circuitry that is not yet well understood (also, please see a more detailed model diagram at the following link, bottom page, Figure 1: https://anonymous.4open.science/w/cochstream-project-page-0546/). 

Transformers: We provide three reasons why we think Transformers might serve as a useful approximation for stages of auditory processing:

i) It is well-established that prediction is a neural objective [2] and hence an autoregressive, causal Transformer is a reasonable candidate hypothesis for prediction-based processing in auditory processing at an algorithmic level, but likely not at the implementation-level [3].

ii) Transformers are incredibly powerful for many biologically-plausible tasks, and we wanted to build a model capable of handling various audio-based tasks that humans perform. In the perfect world, we would love to develop a model that perfectly aligns with neural circuitry at a fine-grained implementation-level and is powerful for tasks, but given that we do not know precisely the implementation-level details of auditory processing yet, we think the Transformer serves as a powerful stand-in.

iii) A large transformer with an autoregressive training objective can be viewed as evolving the  circuitry that a biological brain is born with. Computational models did not undergo evolution in the same way as humans (or other species) did – hence, a large training phase where a generic model architecture is exposed to statistical regularities of the world (in this case, speech) may serve to wire up function-specific architecture similar to what a brain is born with.

Finally, we emphasize–as also mentioned in the discussion of our paper–that “We by no means claim that CochStream is a perfect biologically-inspired model, but it is a critical step in the right direction.”.


Weakness 2. Transformer-APC comparison: We agree that a direct comparison between a Transformer-based APC model and CochStream would be useful, but given the large amount of additional experiments the reviewers have asked for and our limited resources we will unfortunately not be able to run all of them. Instead, we have included the following sentence in the paper discussion to make it apparent that we do not claim a superior input representation but rather an advancement of previous work:

“WavCoch could operate with a mel spectrogram as the target instead of a cochleagram. Our work does not claim superiority of a cochleagram over a mel spectrogram. Instead, the main novelty from WavCoch lies in encoding one representation (in this case, the waveform) into another representation known to be computed in the auditory processing hierarchy (cochleagram) and “probing” an intermediary representation as discrete tokens. We chose the cochleagram for its biological plausibility. Hence, CochStream resembles the APC model [4], but with a quantized input and a Transformer architecture that enables efficient training and scalability.”

Nevertheless, as discussed below we will dedicate most of our resources to generating a version of our model trained purely on open-sourced data to improve the strength of several comparisons (see response to Weakness 7). 

Weakness 3. Effectiveness of separate vs. integrated discretization. We believe our method offers conceptual and engineering advantages.
*Conceptually*, WavCoch can be viewed as mimicking the human cochlea. Since a core goal of our framework is to ultimately get as close as possible to the true human auditory processing stream, we believe that building a separate stage for the inner ear is critical (and this stage can separately be optimized with additional details based on the cochlear basilar membrane etc., and critically, the downstream CochStream Transformer model can similarly be updated). 
*From an engineering standpoint*, a separate quantization model, which converts the high dimensional raw audio signal into a much lower dimensional token representation, allows us to train the large sequence model by loading the tokens directly into RAM, significantly improving training efficiency and scalability. With the limited hardware available to us, scaling an online tokenization system would be practically infeasible.

Weakness 4. Unclear advantage of cochleagram representation.
WavCoch could operate with a mel spectrogram as the target instead of a cochleagram. We do not make any claims about the superiority of a cochleagram over a mel spectrogram (we have clarified this in the revised manuscript). Instead, the main novelty from WavCoch lies in encoding one representation (in this case, the waveform) into another representation known to be computed in the auditory processing hierarchy (cochleagram) and “probing” an intermediary representation as discrete tokens. We chose the cochleagram for its biological plausibility.

Please see Weakness 2 above for the paragraph we have included in the revised discussion.

Weakness 5. Efficiency claims not verified. 

First, we want to emphasize the differences between our cochlear tokens and HuBERT tokens: Our cochlear tokens serve as a tokenization of the audio input, with the aim of preserving useful information in the raw waveform. Conversely, HuBERT tokens are derived from the embeddings of a deep layer of the model through K-means clustering with the goal of obtaining time-integrated semantic tokens. Hence, while HuBERT tokens have a lower rate than ours (50hz vs. 197hz) they are not directly comparable. Instead, a K-means clustering of 4 consecutive CochStream embeddings from a deep model layer (not cochlear tokens) could be compared to the HuBERT tokens. We believe that our cochlear tokens are more comparable to neural codec tokens such as SoundStream. 

Second, we are conducting two separate experiments to verify the efficiency of cochlear tokens: i) We are investigating the effects of vocabulary size on L2 prediction error on an OOD test set to ensure that your cochlear tokens are indeed efficient. ii) We are currently training a vocoder to evaluate the various encoders to ensure that we are selecting an optimal compression ratio. 

Finally, we note that in our implementation the cochleagram transformation is not the bottleneck during WavCoch training, rendering any efficiency gains over a mel spectrogram unhelpful in terms of overall efficiency (see also Weakness 4 above). 

Weakness 6. Inconsistent and incomplete training details.
We deeply apologize for the inconsistencies in the training details. We have fixed and revised the model size and description across the entire manuscript – the error was due to an earlier model version with a larger vocabulary size (which we managed to decrease down to 16,384 as in the current model, and hence do not use the former model version anymore).

To sum up, **WavCoch** was trained on 500 hours of data and the parameter count is 5M. **CochStream-base** was trained on 500 hours (different from WavCoch) and the parameter count is 97M. **CochStream-large** was trained on 50K hours (and the parameter count is 1,300M. 

Please see the Weakness 7 below for response regarding training data. 

Weakness 7. Unfair comparisons.
We plan to release the audio dataset as part of a future work, but the release of the data have been delayed. We acknowledge that this is not an ideal situation and offer a solution to this incompleteness: To provide a more fair baseline and enable full reproducibility, we are currently training our entire pipeline using fully open-source datasets (LibriSpeech). We plan on having preliminary results by the end of this rebuttal period and will provide a full analysis in the final manuscript

Weakness 8. Improper references.
We thank the reviewer for pointing this out, and will definitely fix this in the final manuscript.



References
[1] Levy, William B., and Robert A. Baxter. "Energy efficient neural codes." Neural computation 8.3 (1996): 531-543.
[2] Spratling, Michael W. "A review of predictive coding algorithms." Brain and cognition 112 (2017): 92-97.
[3] Marr, David, and Tomaso Poggio. "From understanding computation to understanding neural circuitry." (1976).
[4] Chung, Yu-An, and James Glass. "Generative pre-training for speech with autoregressive predictive coding." ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.



Second Response:




Global Response:

Writing / Diagram Results:
Cochlear diagram + general model architecture diagram + more explanation (R1) 
Talk about what our model is good/bad at (R3)
Explain in more detail why autoregressive (R3)
Argue for readout/transformer plausibility (R2, R4)
Argue why we don’t use mel (R2, R4)
Fix references (R4)

Training / Code Results:
Better numbers on SUPERB / explanation as to why the results are what they are (R1, R2)
Eval Encodec/DAC/Soundstream on similarity and probing (R2)
Make graph of reconstruction quality vs. codebook size
Explore codebook collapse a bit ? (R2)
Add plots about codebook mapping to phonemes/words etc.
Vocoder (R2, R3)
More human/subjective evals (R3)
Naturalness ratings of predicted audio? Compare with HuBERT? (R3)
If including more SUPERB, make it focused on non-content stuff (R3) 


Reviews:
R1 ipew
Summary:
This paper presents CochStream, a biologically-inspired two-stage framework for speech representation learning. The model leverages human cochlear-inspired audio processing to create a discrete "cochlear token" representation, which it then feeds into an autoregressive Transformer model to predict future tokens. This approach differs from traditional signal-reconstruction and contrastive models, aiming instead for a representation that captures the hierarchical structure of human auditory processing. Experimental results show that CochStream performs competitively on phoneme recognition, word decoding, and lexical semantics tasks, often surpassing existing baselines on the SUPERB benchmark.
Soundness: 2: fair
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The model's biologically-inspired cochlear token framework is well-aligned with human auditory processing, making it a promising approach for more human-like and interpretable speech representations.
The authors validate the model’s versatility across various tasks (phoneme/word decoding, lexical semantics) and benchmarks (e.g., SUPERB), demonstrating its competitive performance and interpretability advantages over existing models.
Weaknesses:
The paper lacks formulas and a clear explanation of the cochlear representation, as well as a model architecture diagram. This makes it difficult to understand how the cochlear representation is converted into audio and how it compares to or provides advantages over the mel representation. A more thorough theoretical or visual explanation of the cochlear encoding process would enhance clarity.
MAYBE ASAP
The experimental results on linear probing performance for phonemes and words on the TIMIT dataset are insufficient. Notably, CochStream-base is not compared with similarly parameterized models such as WavLM-base or wav2vec2-base, which limits the ability to assess CochStream's true effectiveness relative to models of similar scale. Additionally, CochStream-large demonstrates only average performance in Word Decoding. The model's embedding performance on various downstream tasks in the SUPERB benchmark also does not clearly showcase a significant advantage over existing models.
Questions:
Why is section 2.1.4 OBTAINING COCHSTREAM EMBEDDINGS empty?
ASAP
Due to the lack of information in OBTAINING COCHSTREAM EMBEDDINGS, I am unsure how the representation predicted by this GPT-style autoregressive Transformer compares in speed to representations from models like HuBERT, which use masked prediction. Could the authors clarify this aspect?
ASAP

R2 npk7
Summary:
This paper introduces CochStream, a biologically-inspired model for representing speech by predicting sequences based on auditory representations akin to the human cochlea. The model operates in two stages. (1) WavCoch Encoding: The first stage transforms raw audio into a time-frequency cochleagram (a cochlea-inspired representation), effectively converting the continuous audio into discrete "cochlear tokens." (2) Autoregressive Prediction with CochStream: Using these tokens, CochStream—an autoregressive Transformer—predicts the next token in the sequence, creating a generative model that can continue audio sequences and interpret speech patterns.
Soundness: 3: good
Presentation: 3: good
Contribution: 2: fair
Strengths:
CochStream explores an audio representation generation method inspired by the human cochlea.
CochStream was evaluated on the SUPERB benchmark for tasks such as speech recognition, intent classification, and speech separation.
The model can visualize predictions in cochleagram form, offering interpretable insights into its speech representations.
Weaknesses:
The baseline model seems a bit weak and should try to incorporate newer and more powerful models such as Whisper; additionally, CochStream claims to have the appearance of acoustic information, so that should be compared with models that aim at audio reconstruction such as Encodec, DAC, Soundstream;

Whisper is not self-supervised.. Eval Soundstream/Encodec on sSIMI and linear probing?
Based on the results in Tables 1 and 3, the performance improvement of CochStream over the baseline models appears limited. I would like the authors to further clarify the main advantages of CochStream compared to traditional approaches.

Emphasize roll-outs!! Vocoder? Tool that you can use to interpret things.. Better results on SUPERB? 
The design of CochStream reminds me of recent popular single-codebook audio reconstruction approaches, such as Single-Codec and WavTokenizer. I noticed that CochStream has a vocabulary size of 16,384. Have the authors explored the impact of vocabulary size on performance and the vocabulary utilization rate? Is there any occurrence of collapse?
Our vocabulary utilization rate is great. Token/phoneme. 

CochStream seems to complete wav continuation based on a seed. I’m curious whether it has the ability to generate long-sequence audio and how effective it is.

Vocoder + next paper. 

In fact, CochStream aims to explore audio representation from a biological perspective, but I believe that this simple autoregressive modeling approach cannot adequately simulate the complex processing mechanisms of biological systems.

Transformer: evolving and training circuits.. 

We by no means claim that CochStream is a perfect biologically-inspired model, but it is a critical step in the right direction. CochStream uses a Transformer architecture—a choice driven by its proven efficacy for sequence-based tasks—which might have properties that make it incompatible with biological neurons
Questions:
See details in Weaknesses.
R3 6c7S
Summary:
The paper proposes a self-supervised learning (SSL) framework to learn generic speech representations using cochlear tokens derived from a biologically inspired waveform-to-cochleagram transformation, referred to as WavCoch. These cochlear tokens serve as input representations for an auto-regressive model, CochStream, which aims to emulate the human auditory processing stream.
Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
The paper is well-written, with a clear and interesting motivation rooted in mimicking the human auditory system. The authors’ use of cochleograms as intermediate acoustic representations is compelling, and the performance of these biologically inspired features on downstream tasks is promising. The results show that hand-crafted, biologically motivated features can indeed achieve competitive performance, which could have significant implications for the design of speech processing systems.
Weaknesses:
While the paper achieves competitive performance on benchmark tasks, it relies on general-purpose, objective ML measures to validate the proposed features. This focus shifts away from the original biological motivation to a more standard ML performance evaluation. For researchers focused on ML, data-driven features generally remain more attractive due to better performance across tasks and the lack of a need for hand-crafting input features. To realign with the biological motivation, the paper would benefit from additional biologically relevant evaluations, such as subjective metrics from human listening tests, to better demonstrate the utility of these handcrafted features for tasks where biological relevance is key.

More cognitively involved eval stuff
Human listening stuff ask?? ASAP

Questions:
Performance on Non-Content Tasks: Given that WavCoch is designed based on human auditory processing, it would be expected to excel in tasks that are less content-focused. The promising emotion recognition results align with this expectation, but the lower performance on speaker identification is less encouraging. Could the authors consider discussing this disparity to clarify the strengths and limitations of the cochleogram-based representations for non-content-focused tasks.
Run more SUPERB evals? One more data point? 

Speech Continuation Capability: In Section 3.3, the authors include examples to illustrate CochStream’s speech continuation ability. However, assessing this capability solely by visual inspection of cochleograms is challenging without quantitative metrics or synthesized speech outputs. As the framework is intended to mimic human auditory processing, could the authors incorporate a subjective evaluation from human annotators on synthesized outputs to strengthen the claims of biologically motivated capabilities?
Naturalness ratings / BERT causal stuff? 
Motivation for Auto-Regressive Training: The paper currently lacks clarity on why an auto-regressive model is preferred over, for instance, a BERT-style masking training approach. Is the primary intent to enable speech continuation? This could be clarified. Providing further justification here would add clarity to the design decisions.
ASAP



R4 Nhso

Summary:
The paper introduces a two-stage framework for speech representation learning, drawing on biologically inspired cochlear representations and autoregressive modeling. The first stage, WavCoch, is a vector-quantized encoder designed to convert an STFT spectrogram into a cochleagram representation via MLP layers, a 14-bit Lookup Free Quantization (LFQ) layer, and convolutional layers. The output of the LFQ referred to as “cochlear tokens,” serves as the input for the second stage. The second stage, CochStream, is a GPT-style autoregressive transformer trained to predict the next cochlear token.
To evaluate the effectiveness of these representations, the authors assess CochStream on tasks such as linear phoneme and word probing, lexical semantic similarity, and several tasks from the SUPERB benchmark. CochStream shows excellent performance in lexical semantic similarity and demonstrates competitive results across other tasks, establishing it as a strong contender among state-of-the-art speech representation learning models.
Soundness: 2: fair
Presentation: 2: fair
Contribution: 2: fair

Strengths:
Originality: The paper takes an interesting approach by introducing a cochlear-inspired representation (cochleagram) for speech modeling, which is relatively uncommon in the field. 
Quality: CochStream demonstrates excellent performance on certain tasks, particularly lexical-semantic similarity, suggesting that the model captures semantic features effectively. Additionally, the inclusion of various benchmarks and comparisons to other models is a positive aspect, as it provides a well-rounded view of how CochStream performs across different types of tasks, even if it doesn’t consistently outperform alternatives.
Clarity: The paper does a good job of describing the high-level design of the WavCoch and CochStream stages, making it reasonably easy to follow the process from quantization to autoregressive modeling. However, some clarity is lost due to inconsistencies in reported training details, which might affect reproducibility.
Significance: By aiming for a biologically-inspired design, the paper contributes to the broader conversation on incorporating more natural processing methods in machine learning. If refined, the framework could be a step toward more interpretable speech models and potentially inspire future work in biologically plausible approaches.
Weaknesses:
Biological Inspiration: While the cochleagram representation mimics some aspects of auditory processing, most other components, including the Lookup-Free Quantization (LFQ) and the Transformer, lack biological basis. This raises questions about the validity of the biological inspiration claim.
LFQ can be viewed as a readout. 14-bit thing is the plausible thing. Rate codes. 14-bit represents a discrete state. 

ASAP

Transformer-APC Comparison: Since CochStream resembles a Transformer-based APC model but with cochlear tokens as input, a direct comparison with an APC trained on the same dataset would strengthen the claim that cochlear tokens are superior to filterbank features or other standard inputs. Without this, the benefit of cochlear tokens over more conventional features is less evident.
Effectiveness of Separate vs. Integrated Discretization: Unlike VQ-APC, which integrates discretization within its structure, CochStream uses a separate WavCoch stage. Yet, no experiments justify the choice of a separate discretization stage over an integrated approach. A direct comparison could validate whether the proposed method offers advantages.
Unclear Advantage of Cochleagram Representation: The benefit of using a cochleagram over other perceptually inspired representations, like the mel-spectrogram, is not fully demonstrated. Although the paper claims that cochleagram outputs improve interpretability, this interpretability could also apply to other representations. Moreover, the Transformer architecture remains largely a black box, limiting interpretability beyond the output of the WavCoch model.
Yes, transformer blocks are always annoying. But the output is at least good.. It is nowhere in our paper that we are better than mel.

Efficiency Claims Not Verified: While the paper claims efficiency as a strength of cochlear tokens, there is no quantitative analysis to support this. For instance, CochStream’s tokens are sampled at a higher rate (197Hz) than HuBERT’s (50Hz), and cochleagram computation is more costly than mel-spectrograms, albeit only at training time. These factors suggest that cochlear tokens may not be as efficient as implied.

Learned output of HuBERT compared with the input tokens.. ASAP. 
Inconsistent and Incomplete Training Details: Key details about the training datasets and parameters are lacking or inconsistent, which raises concerns about reproducibility.
No citation or details are provided about the 50,000-hour dataset, leaving ambiguity around its source. Using standard datasets like LibriSpeech or LibriLight for English speech tasks would enhance reproducibility.
In Section 2.1.3, it’s mentioned that WavCoch was trained on 500 hours while CochStream was trained on 50,000 hours, yet CochStream base is reported as trained on 500 hours in the tables.
Table 2 states CochStream base has 135 million parameters, which contradicts other descriptions in the paper, leading to further confusion.
We want to release data properly. So it will happen. For reproducibility, we also trained a model on LibriSpeech 
Unfair Comparisons: There are no baselines that are trained on the same dataset, hence it is difficult to put the results into perspective. It remains unclear whether the performance improvements come from the dataset, the transformer configuration or the input representations.
Now we just compare with other stuff on Librispeech. 
Improper References: Several references are cited from the arxiv versions instead of the original published venues. Also, serveral citations lack any details about conference/journal where they were published.

Sorry.



CochStream Speech


TODO Klemen By Next Friday:
Package model in HuggingFace
Start Superb Eval
Analyze Cochstream Embedding Space
Look at unit magnitudes
Simplest noise WavCoch training thing? 
Train WavCoch mixing all perturbations above
Potentially implement ElaiaNet
Train GPT2-style llm with varying context length
32 tokens
64 tokens
128 tokens
1024 tokens [too muchhhh]



Effects of noise on speech comprehension, processing:
Leading and Following - Zhang 2023
EEG study of effects of noise on speech processing
Tested 3 conditions (no noise, 3 dB, -3 dB) of spectrally matched (to the source audio) stationary noise
The TLDR is that added noise increased the latency of semantic level and acoustic level responses
Effects of linguistic context and noise type on speech comprehension Fitzgerald 2024
They test the effects of natural (babble) background noise and spectrally matched stationary noise on speech comprehension
Spectrally matched static noise had less adversarial effects than actual natural noise, though not by much
Adding noise to inputs in audio processing ANNs
WavLM - Chen 22
Sometimes adds a secondary speech utterance overlapping the original source audio (for no more than 50% of the total duration of the audio, so the model can figure out which is the primary speaker) and mixes the two with a random mixing energy ratio randomly sampled from the distribution (-5, 5)
Other times adds a noise from the DNS set (see bellow) with a mixing ratio in randomly sampled from the distribution (-5, 20)
Randomly sample SNR ratio, mix noise with signal.
DNS: Deep Noise Suppression Challenge - Reddy 21
This paper proposes a challenge for denoising audio optimized for human perception, releases noise datasets
Naturalistic
It also introduces this metric: DNSMOS for measuring speech quality after noise suppression - perhaps we could use this to measure how well cochstream denoises


Meeting 20241101

TODO Klemen By Next Friday:
Make plot of reconstruction error vs. token distance vs. (noise / volume / pitch)
Test different model sizes on this plot
Simplest noise WavCoch training thing? 
GT + KK: do quick google on noise types???
Train WavCoch using white noise with SNR in range (-5, 20)
Train WavCoch using background noise with mixing ratio in range (-5, 5)
Train WavCoch using pitch up and down perturbation
Train WavCoch mixing all perturbations above
Potentially implement ElaiaNet
Package model in HuggingFace
Train Vocoder on Cochstream Tokens



ElaíaNet


If we want to build a system robust to noise while keeping it biologically plausible we can utilize the fact that we get slightly different audio inputs on each ear
In fact the loudness of the sound can differ by as much as 30dB, while the time delay is at most 0.5ms
Each ear can get its own high frequency cochleagram, predict tokens, then the goal is to decode the opposite ear’s input stream - swapped prediction problem
We can pass a small 16 dimensional vector between the two ears to align the volume and timing

References:

This primer on human hearing from UIUC
Physics, ecological acoustics and the auditory system  [McDermott 24]


WavCoch codes / phonemes / frequency etc.

Higher number: more pure. 



Pre preprinting TECHNICAL:
Plot phoneme conf matrix on diff color scheme and check sentence again
Figure 2: make x-axis label bigger and y in B
Use title capitalization in legends in both Figure 2 and 3
Figure 2: sSIMI check which set it is on
Figure 2: perhaps other color scheme like jet or something??
Unify whether we talk about correlation or distance
GT: it is cosine distance between embeddings → get one num. For humans we already have a number. Then we take the spearman between those two arrays.
Add bold font / color coding in tables in the final version
Go over “she” plot again
Re-check all results after we re-run model
Check seed 3 and 4 in she plot 

More ideas: Stuff to add to revised speech preprint and/or LLM preprint
+ the Dutch girls paper (Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0)
For language paper: Tokenization sucks, e.g. Giunelli 2024, AlKhamissi 2023. Let’s just not tokenize all along.. 
Sylber: Syllabic Embedding Representation of Speech from Raw Audio
Other models to cite: 
RNN-t
triplet loss based prediction
time contrastive networks
DASB - Discrete Audio and Speech Benchmark?
Training on cochleagrams rather than raw waveforms can be thought of as a built-in prior, ensuring that the input to the network mimics aspects of the peripheral auditory system to some extent. Other work has found that the constraint of a hard-coded cochlear model is useful in producing models that resemble human hearing (Saddler et al., 2021).
To generate cochleagrams, waveforms were passed through a bank of 160 bandpass filters. The filter bank was chosen to approximate frequency selectivity of the cochlea, subject to the constraint of being easily invertible
Each filter was intended to model the response of a different point along the
basilar membrane.




More Future Work :)
BabyView data
Training it against other datasets. 
Question is, what would we evaluate it on? 
You train on X data, and then can you do Y? What do you “get for free” given this input? 
Is there an effect of “curriculum learning”? 
Potential BM: Lavechin 2023 BabySLM (based on CHILDES) 

CochStream Language
Things to investigate:
WavCoch – what’s the best version of this??
Variable length temporal tokens? 
Architectural ablations on small-parameter models

Motivation: We want to learn a model that does language (i.e., knowledge of words and how they compose together) straight from audio (cochlear tokens).

Model: Speech to speech. But we want to be able to produce an output, so we want to have an articulatory model. 75minutes of i) waveform recordings, and ii) 200 samples per second ephys recordings (~50 channels). Articulatory tokens (the output of the channels, discretized). From articulatory tokens to actual audio: HiFiGAN. 
So it would function as a regularizer/constraint for what can be done in the output. 

Context length: 30s ? 

Benchmarks:
Perplexity
Requires transcribing the data (Lakhotia 2021) 
Spot-the-word accuracy from Zero Resource 2021 (Nguyen 2021)
Detecting a real word form a pair of short utterances like “brick” and “blick”
sWUGGY (from Zerospeech Challenge 2021)
List of pairs of word/non-word synthesized with the Google TTS API and filtered for the words that are in the LibriSpeech training set.
sBLIMP (from Zerospeech Challenge 2021)
List of pairs of syntactically correct/incorrect synthesized sentences.
sSIMI (from Zerospeech Challenge 2021)
Lexical similarity
SpeechGLUE (Ashihara 2023)
Acceptability, sentiment, semantic equivalence, entailment etc. 
Stop: spoken task oriented semantic parsing (Tomasello 2023)
Spoken Story Cloze (Hassid 2023)
5 sentence stories with distractor versions where the last sentence does not make sense.
STSP (Nguyen 2024)
IMPLI (Figurative language) (Stowe 2022).
Big, text-based NLI benchmark for figurative language.
EPIC (Irony corpus) (Frenda 2023)
Text-based.
EmphAssess (Prosodic benchmark)
Reproduce prosodic emphasis
SLUE Phase-2 (Shon 2023)
Question-answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. 
SUPERB
General benchmark of various aspects of speech understanding
In particular intent classification (IC) and some others are informative for extracting information beyond word identity from the speech signal 

Control model that is text-based?
On the transcribed spoken corpus /
Same number of parameters, doesn’t seem fair? Maybe make the model “big enough to not overfit” ?
Efficiency of training


Control models on SpeechGLUE:



More Future Work :)
Finetuning the base CochStream-Speech model on “instruction tuning” or other “finishing” datasets to have it behave more conversationally
Doing AI safety evals on CochStream-Speech and the general domain of “speech only LM alignment”
Collect data on the Cloze task in human language regions ?? 


To-do when Klemen is in Cambridge
Go to Verveine 🍵

CochStream language: brainstorm and write out specific benchmarks 
Articulatory model
Linguistic benchmarking
Go over boring shit with the ICLR paper, i.e., lingering things → make plan for sending it out to people
Talk about prediction/generation. 
For paper: we could talk about that models like w2v and hubert are not used for generation and that unit2unit models are fit on top of those – in our case we don’t need additional models to do “next-audio” prediction → GT add to discussion. 
GAN to understand the rollouts?
Code release / weights → getting model on HF is priority #1
Project website
How to use the model / where are the weights 
Include main, final figures from the preprint “main findings” + roll-outs
Interactive demo
SUPERB → when we have final version of model, submit to platform? 
KK shares repo, GT will take a look etc, we need the rest of the tasks
sSIMI https://github.com/zerospeech/zerospeech2021_baseline – probably not!!
Anon website – GT will do a pass and KK will update seed 4


Discuss / draft out outlines for future papers / ideas 
Depth vs. width
Divergent or convergent with humans? 
Distillation
Evolution – language is already a by-product of evolution, perception is not?? 
Language – width??
Classes of functions
Talk about topography
Babyview

WavCoch inv ideas:
Cochleagram reconstruction 
Add noise and see how tokens change
See how tokens are specific to certain phonemic clusters
Change pitch of speech

Pre ICLR rebuttal: 

Project pitches (extra stuff we want to do)
acoustic unit discovery at various levels (we discussed this, but kind of let it go because we didn't think it was our main focus, but there should be quite a few benchmarks etc to show that a model like that learns linguistic units at many levels -- this could be tied to hierarchies as well)
hierarchy project: deeply probe the layers of the model, thorough comparison to other models
I know she's worked on untrained models before, there could be some cool links there... which inductive biases does an untrained model have, and what does it learn very very early in training (perhaps merged with babyview data stuff?)
run psycholinguistic experiment on our model
20241011 (Greta Day)
We are training a Deep version of CochStream on a much longer schedule
96 layers, 1024 embedding dimensions, still 1.3B parameters
Training schedule is set to 500k steps
Loss is going down faster than original model
Deeper is better? What do we think about this?
Perhaps we can cut off the latter half of the model and use the first half as the encoder? Make it smaller and more efficient!



The more we train the model the better the probing accuracy gets
The more we train the model the earlier the best representation for word encoding emerges
Perhaps once the model is fully trained we will decode form a very early layer (or maybe the middle point)
If the loss continues to follow this trend we will have probing performance in the mid to high 80s once the model converges

Investigation into WavCoch
Our reconstruction quality (and thus to some extent the amount of information we can operate with) is constrained by 2 factors:
Quantization rate (14 bit) and cochleagram dimension 211
Increasing the number of bits, reducing coch dimension can both lead do improved reconstructions — do we care?

New Quantizer - Based On Bio Plausible Articulation Model ?
Neuron paper about this data/model: https://www.cell.com/neuron/pdfExtended/S0896-6273(18)30339-8
Articulatory Encodec: https://arxiv.org/pdf/2406.12998
MaxSimple & successors
Post-Mortem ICLR 20241004
CochStream speech
KK is training more; hopefully we can get better numbers
GT will look at rollouts → website? 
SUPERB: more tasks? 
KK will take a look at depth/width (“ablation”)

CochStream language (target Feb 15?-- aim to get paper READY in Jan)
Checking whether our cochleagrams are optimal
WavCoch and vocoder merge?
Think through what we want to demonstrate
Knowledge of syntax etc 
“Babbling-loop” that generates speech
Architectural changes to enforce functional subdivisions 
Diffusion? 


Abstract 20240925
We introduce a biologically-inspired model [NAME?] for encoding speech that learns information about different granularities of speech through a simple autoregressive prediction objective.

Our approach is inspired by the human auditory processing hierarchy. The first stage of our model transforms the raw audio waveform to a time-frequency representation inspired by the human cochlea, with an intermediary step that effectively discretizes the audio representations. The second stage of our model learns a simple autoregressive sequence model over the discretized audio input.

We demonstrate that our model learns meaningful representations of phonemes, word identities, and lexical semantic similarity. In addition, our model shows competitive performance on several audio tasks from the SUPERB benchmark [TODO: on par with]. In addition to our model’s strong representational capabilities, we propose an algorithm for discovering acoustic units of variable length in a self-supervised manner by leveraging the model’s internal attention maps. We demonstrate that the acoustic units derived from our model are robust across different speakers [TODO: delete? and as a proof of concept we train a speech language model on these units which perform better than the HuBERT-base tokenization on a sentence-level ABX task]. Our model provides a novel framework for speech representation learning and acoustic unit discovery, aiming to advance the development of more human-like models that learn language directly from acoustic input.



Meeting 20240925
QuantCoch3
2 Conv filters that you combine (summed! Real and imaginary), followed by 4 layers.  500M parameters
“Manual discrete fourier transform” 
Specs for conv stuff: responses to different frequencies (twiddle). Maps from signal waveform space to 2D in a fourier-like way but not exactly. 
Cochlea goes from raw signal to time bin space of mel spectrograms. 
Mel → coch difference: basically each freq band basically gets reweighted by a different number. 
Two conv outputs are ultimately added together. 
Fourier is usually continuous, but our implementation is discrete (perhaps right terminology would be FFT??)
The Fourier params are a function of the sampling rate and hop length. You need a certain length of input to detect a given frequency: Nyquist. 
Window 1001, sampling rate in our case is 16K, 16000/1001 = ~16 Hz is per second. Human audible range: 20Hz-20kHz (ie. If we oscillate 20 times per second: people can’t hear below 20Hz). Top limit (“the highest frequency that can be accurately captured during sampling”) is Nyquist 16kHz / 2 = 8kHz. Lower is 16KhZ / 1001 = ~16 Hz. Seems not crazy wrt. humans. 
The output of the conv filters should be 501 frequency bins. The input is passed through QuantCoch3 in temporal bins (i.e., a vector). 
The decoder has output size 211 (by 988 temporal bins) so we ultimately downsample in the frequency space)
Each temporal unit is 5ms (80/16000 * 1000)=5ms
After decoder: the 988, 211. 
Decoder consists of 1D convolutional layers. Otherwise weird breaks between adjacent time bins. Makes nicer visualizations. 
We extract one integer per coch time bin. Out of a vocab of 2^14
QM: context windows is now 4096. 

E.g. if audio is 63488/16000 (sr) we have ~4 s audio, then embedding is 782;512.
Post quantizer which is  we have quantized which is [782;512] and indices which is [782] which go up to approx 16K. This means that for every time step we basically have an integer. 
Decoding: back to 782;512; which finally go through conv layers to get back to 782;211


Twiddle factors are tensors of [1001;1001] (first line)
And then we take only the first half of the frequencies, which correspond to the positive frequencies. This is due to symmetry in the Fourier transform. Gives [501;1001]
 
In the context of the Fast Fourier Transform (FFT), twiddle factors are complex numbers used to break down a complicated signal into simpler parts, each corresponding to a single frequency. Twiddle factors work by rotating points around a circle in the complex plane. By applying these rotations systematically, we can figure out how much of each frequency (or "component") is present in the original signal.
DFT can be implemented in conv filters. 
The conv filters reshapes to [501;1;1001]
Window is [1001] → [1;1;1001] (Hann window gives smoothing and avoid annoying edge things) 

Then the conv filters are set as learnable parameters
The rest of the model is: 
So in some way: the twiddle factors are extracted using a conv1d:






Meeting 20240912
Initial
Get big model running! 

Introduction / framing

Cochlea gram framing stuff → read old papers, write, talk to Jenelle/Josh??
In depth cochleagram:

Progressive loss of temporal dynamics from the periphery to the cortex (from Chi 2005)
Another important change in the nature of the neural responses is the emergence of elaborate selectivity to combined spectral and temporal features, selectivity that is typically much more complex than the relatively simple tuning curves and dynamics of auditory-nerve fiber responses (Nelken and Versnel, 2000; Shamma et al., 1993; Edamatsu et al., 1989). 
From Chi 2005 (note, about spectrotemporal model though, not cochleagram specifically!)




The stimulus is encoded by many nerve fibers at once, together, they provide a faithful representation of the stimulus. There are around 30K auditory nerve fibers per ear.

One of the most important attributes of the auditory system is frequency selectivity. Auditory nerve tuning suggests that the cochlea can be thought of as a bank of bandpass filters. 

From Wang Shamma 1994: 

Add on to Jenelle: so it is only the transformation in the cochlea that is the cochleagram transformation? 




To achieve these goals, we first learn a transformation from the audio waveform to a time-frequency representation inspired by the human cochlea \cite{feather2023model} (Figure \ref{fig:schematic}A-C; \textbf{WavCoch} front-end). Note that this approach takes inspiration from neural audio codecs, but instead of reconstructing the \textit{same} signal, we predict another audio representation -- known to be computed in the human auditory processing hierarchy -- the time-frequency cochleagram representation \cite{cognitiveneuro,shamma2001role}. We hypothesize that the representations learned through this transformation will develop inductive biases similar to those of the human cochlea, benefiting the model's ability to process diverse speech patterns efficiently. We probe the representations in the penultimate block of our WavCoch model to obtain an input representation for downstream prediction.

Introduce QuantCoch overall → greta looks at code + we need to update methods

Results: probing (phoneme, word, lexical)
Run BMs on new model. 
Exhaust pooling+layers [also for control models] – weighted sum thing?
More fine-grained temporal shift probing?

Results: SUPERB
Better scores for our model?

Results: autoregressive roll-outs
We already have qualitative plots and we can show more on an anon website 
We can model a whole distribution of plausible things…
For quantification: ASR? [talk more]

Results: units?
Do we even want to show this??
The claim would be that units at varying sizes can be decoded from our attention maps. But we need a way of quantifying that. Norman-Haignere showed that it’s all about time-scales and not unit coding in auditory cortex. Can we show a scatter of time-scales?? [maybe – think more about this, perhaps try to quantify time-scales?]

Other:
Figure out which baselines to include!!
Anonymous website! 
Psychophysics / we can prompt this in more clever ways





Meeting 20240829 - Post-mortem-secundus
What is the potential re-submission plan?
Neurips notification is Sep 25, 2024
ICLR deadline is Oct 2, 2024
Too little time to do a good turnaround job, so we should decide today if we plan to re-submit
Re-submission direction:
We can double down on the neuro-plausibility
Go hard on representational learning 

Lego blocks of what aspects we can put in the paper(s)

Proposed new architecture / architecture components
QuantCoch
QuantMax

QuantCoch is much more bio plausible (cf. reconstruction)
Enables interfacing “raw” signals to a strong learner

Representation learning
Linear probing:
Phoneme decoding
Word decoding
Lexical semantic decoding
SUPERB
Qualitative evals (“she” plot etc) [needs more examples etc either way]


Beating the INPUT to AudioLM
QuantMax has the correct “substrate” to learn from dev plausible data, while another architecture does not.

Acoustic unit discovery
Attention-based algorithm 

Not fixed units etc

LUM
Sentence-level ABX
Intra-speaker robustness
(Vocoder and qualitative evals of generating speech)


Story line of paper

Bio/neuro: Inspired from (human) biology, we propose this new architecture. We show that it performs well on audio representation learning tasks. 
Arguments: hours of training data. 
Scatter of number of training data versus performance
A punchline about prediction performance
New capability to run cognitive experiments better 
Argument: simplicity. This is a super simple objective, but look at all the things that are learned across layers. 

Better acoustic unit: We create an audio representation model with the core goal of obtaining acoustic units that are not “hard-coded”. We show that those units are better LUM substrates. 
Arguments: we can segment out units at different scales
Need to evaluate what the bounds correspond to
Dive through the stack of layers [a story of auditory hierarchies] [we also need to show that other models don’t have this]

The unique part of our model is that it CAN PREDICT and roll-out things. Only the acoustic ones can do this.. 
Mess with the attention heads 
Quantify “she” plot? 


TODO: 
Plot of number of hours / model size vs. model performance
To answer question about efficiency of learning
Layer wise probing results of representations in wav2vec++, HuBERT++ and QuanMax
We need to figure out exactly which models we are pitting ourselves against
Look at linear probe performance of these models + attention head analysis



Meeting 20240709 
What is being learned in the waveform to cochleagram transformation? 
Controlled comparisons / ablations: contrastive objective. 
Untrained / earlier checkpoints and surprisal
Meeting 20240702 - Greta Day
Misc meeting notes:
QuantCoch is great because it actually learns a transformation from waveform to spectrogram, so learning the right invariances. It is not really an autoencoder. 
It is small!! 
Input: is a time signal, and we can process arbitrary length signals. It is very local. Similar to FFT. Maximal aggregation window 1000 samples wide: 1/16th s (~60ms). The further down the model, the wider it can look. 5s chunks (but the clips can be reconstructed to be nicely “stitched” into larger chunks)
Contrastive learning: leveraging the different aspects of the signal. Wav and coch are ultimately the same. Instead of predicting the cochleagram, one could contrast into the cochleagram. 
Rotary embeddings: provides more locality. Encodes proximity. 1D, causal in one direction. Our sequence of tokens is a 1D, and we have two tokens for each time bin. 
Word boundary detection (not a top priority for *this* paper): 
Old method: surprisal, fixed threshold of 20 with suppression. 
Same sentences from different speakers: get contrastive embeddings? 
Look at attention maps relative to the input sound and see if anything pops out??
Introspective step of “silencing” parts of the input, and seeing what goes with it? 

Plan For July

Klemen:
Week 0 (St. Beatrix’s)
Figure out if we are using QuantMax or QuantMax2 

Week 1 (Due Jul 8)
Meet about ASR + clustering/LUM.
Train QuantMax on LibriSpeech -> 4 Days
ASR (LibriLight) -> 2 Days

Week 2 (Due Jul 15)
Rollout analysis (with ASR) -> 1 Day
Clustering of acoustic units + cluster purity ? -> 1 Day
LUM -> 1 Week
LUM evals: U2T → Look into


Week 3 (Due Jul 22)
Add more stuff to related work / intro
(Revisit discussion)
Fig 1, shows the max time bin surp?
LUM Analysis

Meeting 20240628
Embedding space ft. boundaries: (Once we have fixed model, lower priority)
Merge of surprisal and embedding similarity?? 
Slow learning → distill learning of boundaries? 
Should we try to replicate eval form Bhati 22 ?

Meeting 20240624
Quantization:

5s audio, coch 988; 211 → ultimately 5ms temporal resolution left

Desiderata 1: we want to be able to reconstruct the audio to some extent (does not have to be high resolution at all)
We want the encoding to be somewhat meaningful (instead of a random compression where the decoder learns everything. Encoders are indexers into the representations..)
Evaluation? “AST plot” look, listen.. QM-small, word probe? Run filters on a bunch of data, and then look at responses (mean/variance)? Autoencoder and see what is actually being used? 
Boundaries: leverage downstream embeddings? Shifts in the embedding? One pass through the model. 
	
Meeting 20240612
What do we want from a quantizer that goes from waveform to tokens we can predict?
Efficient / low-bit
Reconstruction is NOT a main objective, but ok if some of the acoustic properties “come for free” (i.e., there’s a link between knowing how phonemes/word go together on a short time-scale and the acoustic features of the speech signal)
Not based on a pre-defined dataset tuning range (the “magical” two constants), probably ok
Local encoding: temporally independent and minimize effect of “easy” predictability from just neighboring frequencies
Meeting 20240610
Number unique words probing → KK rerun, we’ll check
Are baseline levels in Fig 2A weird? → rerun on the full thing? + more points between -200 and 200
Target BMs (for rebuttal)
Yang 2021:
Librispeech PER
ASR WER
Keyword spotting ?
QbE ?
Emotion recognition (IEMOCAP), but some kind?? 
Nguyen 2021:
Phonemic ABX
sWUGGY (pseudo probability)
sBLIMP (pseudo probability)
AudioLM? 
Speech representations ft neural data 
Looks like similar results to us, but more temporal generalization focus (using CPC): https://arxiv.org/pdf/2405.08237 
Klemen’s Presentation about naudio Decoders
ASR Is working really well, we need to think about what data to train on for BM
Vocoder is still not working very well, spectrograms seem to train better than waveforms, naive float re-interpretation quantization representation of either is probably suboptimal
Biggest exploratory direction to pursue is a better quantization scheme - perhaps based on the principle of 
Post-Mortem
Priority List
THINGS WE ABSOLUTELY NEED FOR REBUTTALS
Re-run probing (include pooling + layers + a few seeds??) + do more fine-grained temporal shift probing via the full benchmark (and not word_val?) + check that TIMIT is fresh? -> 1 Week
Train QuantMax on LibriSpeech -> 4 Days
ASR (LibriLight) -> 2 Days
Rollout analysis (with ASR) -> 1 Day
Clustering of acoustic units + cluster purity ? -> 1 Day
LUM -> 1 Week
Boundary strategies:
Simplest solution: just take a left-right streak and look for peak in that value
Growing square: look at variance
Thicker diagonal approach: look at the mean and when it drops
LUM evals: U2T → Look into


Add more stuff to related work / intro
(Revisit discussion)
Fig 1, shows the max time bin surp?

Scientific Debt
High Likelihood:
More extensive linear probing across layers, pooling, etc.
Understand the train/test number of unique words
Accuracy at number of train samples (exhaust words)
ASR with fine tuning on LibriSpeech/LibriVox (standard evaluation)
Analysis of whether our linear probes decode the “rolled out cochleagrams” into meaningful words
Fit “best” probe on TIMIT (and other corpora?)
More in depth analysis of acoustic units (phonetic/word) boundaries
Clustering and/or tokenization analysis of QuantMax embeddings
Cluster purity
Consolidate claims about what makes a good acoustic unit window size
Writing: focus more on variable-length? And perhaps check other k’s?

Medium Likelihood:
Train our model on LibriSpeech (or some other standard dataset to ablate effect of our data)
Sparse speech (either baby or artificially constructed)
Train a LUM model on top of our embeddings to show it makes good training data

Low Likelihood:
Train our model on Soundstream tokens to ablate our tokenization method
Usefulness of ‘staying true’ to the timestamp with a given token (we need to find boundaries too)
The simpler solution is better?!
Model architecture ablations

Feedback On Collaboration
Future Work
Better Bounds:
More flushed out acoustic boundary finding algorithm

Better Tokens:
Fit our own tokenizer? Mixture of auditory gabors? 
Look into spectrotemporal model https://pubmed.ncbi.nlm.nih.gov/16158645/ 
Old school comp. neuro / comp. vision paper on sparse coding of vision patches https://www.sciencedirect.com/science/article/pii/S0042698997001697

Train LUM:
Several approaches to train end to end or to not require tokenization right away
K-means baseline?

Linguistic Benchmarking:
Understand the roll-outs from a more principled (minimal pair?) set of analyses
The following two papers provide some standard benchmarks which we can incorporate into our analysis Nguyen 20 and Yang 21

Dataset Manipulations

Effect of Contrastive Learning on Audio

Learning of Prosodic Features



Conceptual thoughts + future investigations of steps along the way
MaxSimple (boundary selection)
Core premise
Acoustic units are well-structured (i.e., predictive) within but not across acoustic units (as described in Park & Glass, 2007)
It is also cool that we allow this structure to be determined across all frequencies in the input.
Findings:
Float regr model scale much better than quant distr -> we have scaled up to 80M param model and TIMIT val loss keeps going down
Longer context windows help us reduce the loss much more! 
This is quite surprising, as it seems unlikely that 2s vs 5s would help us predict “next bin” much better
Another option is that the longer context model simply uses more compute and that is beneficial to producing better, sharper outputs
The rollout predictions of these models look very interesting because they allow us to pinpoint 
Pros/cons with quant_distr and float_regr
float_regr: L2 loss, on floats
P: Simple heuristic
P: Doesn’t require pre-defined notions of quantization 
C: Less stable (especially for strict autoregressive testing/training?)

quant_distr: quantize (via min/max over the full dataset, log), allows us to sample from distribution of probably next values (either for all frequencies at once or autoregressive squared). Also allows us to compute surprisal in the LLM sense. Here we only quantize the output for starters (not input) to make it more comparable to the float_regr.
P: Quantization stabilizes (our memory is better if we bin stuff into discrete spaces)
P: We can’t transmit signals at infinite rates 
P: Constrains the space 
C: It’s another hyperparameter and we don’t have a good notion of what to select
C: It has the massive matrix that in principle the system has to “hold in memory” (n_freq_bins x n_quant_bins) 

To-do’s
Figure out the strategy of interest (float vs. quant) 
If quant, settle on a quantization parameter
If quant, we need to make sure that the quantization values generalize to other datasets
Scale MaxSimple
Only use TIMIT for validation because we have nice labels
Write a proper metric for evaluating the model
Idea 1: Run it over the TIMIT val set and see how well the boundaries match with word boundaries and with phoneme boundaries using some soft score. ideally we would want to be somehow halfway between the two (problem: we don’t want to necessarily bias it towards a particular unit & think about multi-level predictions)
Idea 2: Evaluate it on across-speaker similarity (hard to do??)
Figure out approach for settling on number of boundaries
Idea 1: Allow for multi-level predictions through boundaries at different granularities 
Idea 2: Use some smoothing kernel (suppression)
Idea 3: Select based on validation set
Different clips have different thresholds (seems suboptimal)
Idea 4: Leverage a combination of entropy and surprisal in some way
Other ideas for making MaxSimple better
Add noise: instantiate random noise of the dimensionality of the input, and add it to the network. By the time we are computing surprisal, add ANOTHER noise vector such that we ensure that the network does not learn to pick up on any noise signal
Perform “roll-out prediction” instead of single-time step prediction
Make the context window smaller: only revisit when we use different datasets
We will aim for max ~5s (when we have our LLM, remember to validate whether shorter boundaries make the LLM worse) 
Link boundary selection to downstream prediction
We can continuously keep adapting the bounds, but it is not a supervision signal


ContraMean (contrastive unit selection)
Core premise
Implement MaxLearn-esque objective on top (push everything within each flexible boundary condition together, and push it further away from the other units)
Run k-means on top (good for comparing to previous work)
Bottleneck (good because it is not a “post-hoc transformation”)
Manifold type of stuff (good for constraints)
→ train GPT style LLM on top 
Option A: add a contrastive thing in the middle 
Option B: just keep adding larger prediction heads on top


To-do’s
Settle on strategy for aggregating to yield acoustic units (current approach: simple mean; other approaches include MaxPooling)
Scale ContraMean
Train a ContraMean model from cochleagrams
Leverage acoustic units in ways beyond k-means
Online quantization bottleneck (continually making the embeddings more discrete)
It’d be even greater if we could learn the size of vocab too
Evaluate the resulting acoustic units 
Idea 1: Cluster purity: Does this cluster always have X phoneme or Y word? Are the phonemes/words in each clusters somehow related (e.g., phoneme class, word length…)?
Depending on how that pans out, we can look at the similarity of a cluster A with B given their phonemic/word labels
Extend to word frequencies: Are certain clusters “high frequency” words? And smaller clusters more infrequent words?
Idea 2: Evaluate to which extent cluster IDs are shared / not shared across speakers
Idea 3: Crop out words and check similarity between clusters: Within-word similarity should be higher than between-word similarity
Idea 4: Train a linear probe to decode embeddings into words 
Idea 5: Shorten the clip and then show that our model actually works well with (much) shorter context (Klemen has a lot of references as to why current models suck on this – actually no, we would need a lot of extra work to do this and we don’t need more work :))

Making sure CM embedding space does not collapse 🥵
Changing objective: 
“Enforcing” positive examples: Different “views” (/noise) of a given sample
Multi-level pred: if you have a bigger unit then the things making up that unit should be more similar → could backfire
Dot prod low [wait until we see first pass of quantization]
Quantization ✅
Explicitly enforcing the full representational space to be used:
Nuclear norm ✅
PCA scaling entropy - Greta (INV Script)
Architectural: 
CM components (transformer, emb size) ??maybe not just small models are nice
Where normalization happens 
Get rid of CM
Look at MS embeddings?




LUM (linguistic prediction)
Core premise
We get boundaries, i.e., segment stuff. Then either we go a contrastive route, or a fully predictive route. We evaluate the models for unit prediction (we need to train a vocoder) or on some linguistic-y tasks.
To-do’s
Train vocoder
Implement linguistic benchmarks 
Check effect of context size 
“Thought” tokens

Initial baseline GT model 
Architecture: GPT, 4 layers, 4 heads, 512 hidden size (context size = 64)
Training tokens: 100-200M regime, GT word bounds (? figure out long words), YouTube 
30,000hours? Assuming 100 words/min 
We want to store continuous chunks of speech (for LLM, despite MaxSimple etc looking at shorter chunks). We need: 1) the waveform, 2) transcription of the given word for each boundary, 3) metadata / audio filename, 4) 
Determining the 30,000hours:
Podcasts/lectures “playlist subset”
Filtering (all subsets should undergo the same filtering): 
1) Hard thresh: if an audio clip is less than 5s (none for playlist subset)
2) Hard thresh: if transcript has more than 5% missing (none for playlist subset) 
3) Hard thresh: if transcript has fewer than 5 words (none for playlist subset)
4) Hard thresh: explicit content 
5) silence/music/other languages in X% of the clip?
We will get the tokens during inference so there is no learning dependent on the other samples in the batch, i.e., batching does not matter for fetching the tokens based on a pre-trained model (but would matter for end-to-end stuff :)) 
Some results regarding this procedure:

Quantization: k-means (32,768), but move on to bottleneck soonish

Training datasets

Naming convention:
100M-MaxSimple
100M-ContraMean
100M-Quantization
100M-LM
100M-Vocoder
100M-Validation

All are 500 hours (about 450 videos each) except 100M-LM which is 100M words or about 20k hours - 7K videos

K-means quantization
To carry out the k-means quantizer process, the two following steps are applied. First, for training, N centroids are learned using a fraction of the training data. After that, at inference time, the output of the quantizer is chosen as the index of the centroid minimizing the X distance
between the input embedding and N centroids learned.


U2T (textual prediction; unit to text)
Aim: Get a sense of what the actual textual outputs are from the predicted LLM acoustic units
Architecture: GPT, 4 layers, 4 heads, 512 hidden size (context size = 64)
Tokens: 32,768

General to-do’s

Priority 1: connect ContraMean + LLM (investigate the quality of ContraMean)
Quantize (via k-means) the ContraMean output (given some ok model) → train LUM → train U2T
Look into the boundaries learned by ContraMean (inv probe, inv sphere, inv acoustic unit )
Quantization to 50K (and 500, 10-20K smaller)? Lower-res version of ContraMean
Is the ContraMean space learnable?
Is it sane (U2T)? 

Priority 2: Make MaxSimple better: 2s? + look at the boundaries ! 1s + bigger batches to make up for the less bins.
We need a link between MaxSimple boundaries and LUM. Better unit discovery → better prediction
Roll-out mechanism provides more info about the “actual” units. 


Priority 3: Learn quantization via bottleneck to avoid yucky big k-means computation over the entire vocab 


Priority 4: End-to-end: MaxSimple, ContraMean, LUM


Merged model (MaxSimpleContraMean and MultiMax)

Architecture overview:
Assuming element window: 5
Future prediction bings: 100
MaxSimple embedding dim of 256
ContraMean embedding dim of 16

MaxSimple:
Input: Cochleagram (5s) [988; 211]. 
Every element in the input is currently a concatenation of 5 timebins (in its most trivial case, an element is just a temporal bin in the cochleagram). This results in a temporal downsampling of the output by a factor of 5 (we also have to trim extra time bins at the end to make the total divisible by 5).
We are predicting 100 timebins (cochleagram bins not elements, i.e., 100 / element_size = elements) into the future at every element, this also means that the temporal axis of the prediction and the output is shortened by 20 = 100/5 (the number of elements corresponding to the future prediction window, since we do not supervise these during training)
Task is next-”element” prediction, i.e., whichever “element size” we decide on 
Outputs:
Predicted cochleagram [177; 100; 211] (177 = (987 // 5) - (100 // 5))
Element embeddings [177; 256] where 256 = MaxSimple emb size
(ultimately we just lose ~.5s of the last part of the audio)

ContraMean:
Input: [177; 256] (i.e., we’re just lacking the very last part of the clip)
In the contrastive loss we need to flatten out across batches, i.e., we’d get [num_batches x num_acoustic_units; 16]
The 177 dim is the “element” dim, but ultimately the information stems from the concatenated 5 consecutive time bins
Irrespective of whichever loss we have (e.g., cosine, nuclear norm…)
I.e., for this computation we flatten out, but for the LUM plug we of course need to retain a sense of linear time (we contrast against all other stuff for the loss, but of course we don’t allow it to update the embeddings trivial based on what it “sees” in the batch)
We also need to select a certain “boundary selection” strategy for initializing the acoustic units (which can then be updated)
Outputs: [num_acoustic_units; 16]

LUM plug (in ContraMean):
To have some kind of “positive pull” in the negative-examples-only ContraMean loss, we use prediction on a larger timeframe. Meaning that for every acoustic unit, we aim to predict the next one (note that this operates on the “abstracted” acoustic units, and in principle has nothing to do with the element window etc).
Implicit “pull”: for every acoustic unit, we want to enforce that the next unit is also predictable, i.e., changing the embedding based on what is likely (which happens at the same time as the contrastive ContraMean loss)


MultiMax:
Pred distribution of future units. I can apply some transform on it such that I can predict it. MLP on top of embeddings of contra mean. Positive example are built by a transform on the current embedding. 
Use distributions of LFQ
Going two ways: LM to CM embedding. Other direction: from CM to LM embeddings. CM defines the targets. 
LM loss is no longer cross-entropy, but would then be a similarity loss. 
LFQ output: code of binary things (64bit, 1 or -1); and then that would refer to indices of everything in the ~60K vocab. Firing in multivariate way: this pattern. 
For LM the indices are supervision.
Predict a distribution of LFQ. 
Multiply by prediction distribution. Move embedding that then becomes the code. 
Embeddings are different and have meaningful sequential order




Plan (prioritized to-do’s)
Implement a linear probe to words 
LLM:
Get stats on youtube (number of videos, words etc) → split it up → make 4 subsets: 0) MaxSimple (~500 hours) 1) ContraMean (~500 hours), 2) K-means subset (~500 hours), 3) Clean val set (~500 hours), 4) Main train subset (the other subsets subtracted from ~30K)
Write quantization (k-means) script on a small subset of the intended train set (and then exclude) NOT FANCY → Write/modify script to tokenize all the training data
Modify/write train script that loads the new cluster-based tokens and feeds them into the LLM + evaluates it 
Understand unit sphere conversation
Examining unit sphere experiments (offline)
Boundaries in MaxSimple / quantization 



Related work
Related work:
https://arxiv.org/pdf/1911.03912 w2v
https://arxiv.org/pdf/1904.03240 Chung 2019

Older work on acoustic unit discovery:

Varadarajan 2008: Unsupervised learning of acoustic sub-word units
Lee & Glass 2012: A Nonparametric Bayesian Approach to Acoustic Model Discovery
Park & Glass 2007: Unsupervised pattern discovery in speech

Lee 2015: Unsupervised Lexicon Discovery from Acoustic Input

Chung 2019: An Unsupervised Autoregressive Model for Speech Representation Learning !! (described below)
Feng 2021: Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent
Subword Discriminative Feature Representation


Kamper 2023: Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring

Different ways in which people learn units for linguistic predictions (learning latent speech and audio representations):
Simple clustering: e.g. based on distances (Rabiner 1979) of autocorrelation features (?) and k-means also based on autocorrelation features (Wilpon 1985)
Self-supervised approaches:
Autoregressive models (objective is to predict the future based on past observations)
Oord 2019 (predicting the future in latent space by powerful autoregressive models. First they map input to latent space, and an autoreg model on top (but use density ratio), predict 12 timesteps into the future)
Bidirectional models (Baevski wav2vec)
For both auto and bidirectional, we then need to learn units, e.g. via surface features or contrastive learning
Liu 2023: DinoSR: extracts contextualized embeddings from the input audio with a teacher network, then runs an online clustering system on the embeddings to yield a machine-discovered phone inventory.
phone inventory, and finally uses the discretized tokens to guide the student network.
VQ-VAE ideas like: Oord 2017

Typically through an encoder: CPC, w2v, HuBERT followed by k-means clustering (Lakhotia et al., 2022; CPC, w2v, HuBERT k-means)

Our model only requires text for the U2T model (or ground-truth validation)




Work on temporal units in auditory cortex
Lerner 2011: approximate word-level processing with 1s +- 0.5s.


Overath 2015: (obs: responses to german speech, i.e. NOT linguistic)
(PT is planum temporale, part of nonprimary auditory cortex). So the ROIs are more ‘speech-y’ regions which is 960ms > 30ms; gets at voxels that respond more to naturalistic speech structure up to 1s). So we can still see that they DO care about +500ms in LH? 

Norman-Haignere et al 2022: category-selective regions (e.g. speech) is > 200ms.
TIMIT.


There is a difference between leveraging a certain time scale to process things, versus learning how to segment them..
Not a big largest segment duration?

Jusczyk and Aslin (1995) demonstrated that infants begin to segment words from fluent speech between 7 and 8 months of age. After familiarizing infants with a pair of novel words, such as ‘cup’ and ‘feet’, infants were then tested on recognition of those words in the context of passages. Results showed that infants listened longer to passages containing familiarized words compared with those that contained novel words.
First use monosyllabic words. And then nonwords. 


Saffran 1996: use four three-syllable words, 270 syllables per minute. 60/270 = 0.22, i.e. three syllable words is 0.66. So there’s sensitivity above 500ms. And the task is transition probabilities ACROSS words. 




Drafted paper sections
Abstract v1

Discovery of variable-length acoustic units through autoregressive prediction of quantized audio

Acoustic unit discovery on discretized representations of perceptually human-like audio via a simple autoregressive objective

Acoustic unit discovery through discretized autoregressive learning

Acoustic unit discovery through autoregressive prediction of quantized cochleagrams

Efficient, variable-length acoustic token discovery through simple prediction on discretized audio 


Humans largely learn language through speech. In contrast, most artificial language models are trained on pre-tokenized text. In this paper, we propose a simple model which learns efficient, variable-length acoustic tokens from raw speech, which can be leveraged for downstream language modeling. Our contributions are threefold: First, we provide an efficient discretization of audio through quantized cochleagrams. Second, we train a simple autoregressive sequence model over the discretized audio. Third, we propose an algorithm for discovering acoustic units of variable-length in a self-supervised manner by leveraging the model’s estimate of uncertainty. We demonstrate that our model learns meaningful representations of phonemes, word identities, and lexical semantic similarity. Finally, we demonstrate the feasibility of our learned acoustic tokens for downstream language modeling. 





Emphasize:
Very simple training objective
Variable length acoustic units and fewer units per second compared to other models
Consistency of acoustic units across words/speakers
Roll-out predictions across time
Feasibility for language modeling
Trained on very little data and a low number of parameters compared to other models.. Scales. 
Our model exhibits good data efficiency, performing competitively to strong baseline models at just 500 hours of training data, while also exhibiting predictable scaling behavior with increases in model and data size.
In summary, we demonstrate a novel framework for obtaining acoustic units for downstream language tasks.
Abstract v0
Humans learn language through speech. 
In contrast, most artificial language models acquire language abilities through pre-tokenized text. 
However, most models that do language well are built based on pre-tokenized text. In this paper, our core goal is to learn efficient, variable-length acoustic tokens from raw speech, which can be leveraged for downstream language modeling. Our contributions are X-fold:  
Emphasize:
Downsampled, efficient input representation 
Variable length acoustic units and fewer units per second compared to other models
Consistency of acoustic units across words/speakers
Roll-out predictions across time
Feasibility for language modeling
Trained on very little data and a low number of parameters compared to other models. Very simple training objective. Scales. 
In summary, we demonstrate a novel framework for obtaining acoustic units for downstream language tasks.




Introduction 
Humans learn language through speech. 
Our core premise is to build models that learn to do language (meaningful linguistic prediction + another example) with minimal built-in / developmentally implausible assumptions (e.g., large-scale frontend; bidirectionality; context windows; in the paper actually list these in a very clear way and contrast it with prior stuff).

Our model is (loosely) inspired by human learning: First, the raw speech waveform passes through the human cochlea (we train on cochleagram input). Second, we have a stage of speech segmentation, i.e. figuring out which parts of the speech go together, without assuming that every chunk of speech needs to be the same duration). Third, motivated by XX (something with contrastive coding), we push our learned segmented speech chunks apart to finally perform linguistic prediction in this space.

So the units are learned. 

Biggest current problems: k-means quantization / defining how large that space is? And that there is no feedback between maxsimple and later stages.

___
Core focus: establishing a good acoustic space to perform linguistic prediction on

To learn the initial “space of boundaries”: we initialize a relatively large model to learn which parts of the audio are likely to go together. We do so on the cochleagram representation, but critically, we implement a “temporal pooling” operation which consists of five timebins of cochleagram information (~25ms) that is then asked to predict an upcoming part of the cochleagram audio (20/100 bins into the future, also ‘aggregated’ chunks). This incentivizes the model to learn which aspects of the audio are easily predictable. We can then leverage that to get an initial sense of what parts of the audio should be segmented together as units for downstream tasks.
To build a more global unit representation: we take the initial boundaries, and add (i.e., dual loss) a secondary objective where we aim to push apart our acoustic units against other acoustic units (across different clips). We goal is to learn a global space that contrasts the whole space of possible speech sounds against each other, and also quantizes/tokenizes the units. To quantize the audio within the contrastive, acoustic boundaries, we enforce a quantization bottleneck (of X bits) that forces the units into a discrete space. However, note that this discrete space does not have any pre-conceived notions of “what a unit should be” – it is fully learned, and does not have to correspond to e.g. ground-truth words, phonemes etc. (provide literature on humans being sensitive to multiple levels of prediction in speech and language + we show that these units can decode XX above chance but…).

Finally, we perform a larger-scale “linguistic” prediction task on top of our acoustic units, and evaluate it via a unit-to-text model, vocoder (?), and the ability of our units to perform decent next-word prediction along with some linguistic benchmarks… 
Showing boundaries on cochleagram
Consistency within-word vs. between-word + phonemes (controls: untrained
Linear probing on phonemes and words (controls: untrained
Units per second vs. [lang model, ABX, decoding of words or something] (controls: would need to meaningfully benchmark other models, HuBERT & others?)
Connection from units to LUM training loss (in the unit space, even though we don’t know what they are, they are useful because they are predictable). 
Compare models that have different ranges of LUM loss and connect it to the metrics above (within vs. between word similarity others…)
Examples of U2T output
(Connect U2T output to some linguistic thing, e.g. larger surprisal for non-acceptable strings to make the point that the U2T output is actually linguistically meaningful)
(Investigate sentence-level embedding space relative to an LLM, e.g., across semantically diverse sentences)

In summary, we contribute a novel framework for audio modeling with fewer built-in, developmentally implausible assumptions. 
Segmenting and predicting units from raw speech 

MultiMax: The motivation for multimax is that on one hand, we need to learn to push things away from each other such that we can operate on discrete symbols that are composable – on the other hand, we need to be able to predict the upcoming token. So in our model we balance out these two properties to have a model that produces acoustic units / language through this divergent processes.



DRAFTY:
Cochleagrams are a variation on spectrograms with filter shapes and widths motivated by human perception. 



Methods 
Pooled-log-quantized cochleagram (PLQC)  
First, we propose a method for efficiently discretizing audio through quantized cochleagrams.
QuantMax takes as input 5 second clips of audio sampled at 16kHz. We then apply a biologically-inspired ‘cochleagram’ transformation (Glasberg 1990; McDermott & Simoncelli, 20211) to convert the 1D signal into a 2D time-frequency domain via Fourier transforms (Feather 2023). This yields a spectrogram with 221 frequency bins and 988 temporal steps. We then apply an average pooling operation on both the time and frequency dimensions with a step size of 5, yielding a 2D input of size 40 x 197. We then apply a 4-bit log quantization scheme to the pooled cochleagram following the equation below:

INSERT EQUATION

Resulting in 2**4 (16) unique values. We then flatten these pooled-log-quantized cochleagrams (PLQC) into a 1 dimensional sequence of 7,880 tokens and compress each successive non-overlapping 4-gram into a single code, with a vocabulary size of 16**4 = 65,536. Therefore, we trade off a larger vocabulary size for a shorter sequence length – while the previous steps of PLQC reduce the input in a lossy manner, this final re-quantization is lossless, resulting in a 1D sequence of 1,970 tokens. 
Architecture:

QuantMax is a GPT-style autoregressive transformer with 12 layers, 12 attention heads, an embedding size of 768 and a vocab size of 65,536 - a total of 126M parameters. It takes as input the token sequence produced by PLQC and predicts the next token in the sequence. We utilize a learned positional embedding and compute the cross-entropy loss between the predicted logits and the true next token in the sequence.

Training: 

We use the AdamW optimizer [CITE] with a peak learning rate of 1e-4 and a 50k step cosine-decay schedule with a 5k step warmup. We use weight decay of 0.1, dropout of 0.2 and norm 1.0 gradient clipping. We train the model with a batch size of 512 on 8 GPUs for 28 hours.

Obtaining acoustic unit bounds:

Obtaining QuantMax embeddings:

We obtain acoustic unit identity by mean pooling the embeddings of all the tokens associated with the corresponding section of the cochleagram (either via ground-truth word bounds or estimated boundaries, TBD). 

A key property of PLQC is that each token can be mapped to a particular frequency and temporal section of the input.




Scaling
In addition to our base model, we train two models of different sizes:

MAKE TABLE 
Num Transformer blocks; embedding size; MLP embedding size (?); num attention heads 
Small:
Medium:
Large: 


Audio dataset curation: 

We curated a dataset of internet audio clips containing high quality naturalistic speech.
Our goal was to generate three such datasets, mostly differing in their size (small: 100M words 💅, medium: 1B words; large: 10B words). In addition, the small dataset was aimed at being the most high quality one. The audio clips were from periodicals/playlists consisting of podcasts and lecture recordings.
To obtain word-level transcriptions of these audio clips, we transcribed the speech using the Whisper large-v2 model from OpenAI (Radford et al., 2022). Numerals and non-standard characters were suppressed in the transcriptions, such that numbers were represented as words and non-standard characters were omitted. Otherwise, default parameters were used. The transcriptions were force-aligned to match the audio files using the WhisperX pipeline (Bain et al., 2023). 

We used the following predefined exclusion criteria for preprocessing/filtering of the audio files/transcripts:
1) Audio clips shorter than 5s
2) Transcripts (associated with a given audio clip) that have more than 5% missing words (i.e., non-transcribed)
3) Transcript that contain fewer than 5 words

In the small dataset subset, no videos were removed based on the above-mentioned three criteria. [Add info on X% of missing words associated with step 2].
Moreover, to ensure that the transcripts were not inappropriate, we silenced the audio associated with the naughty-words package and replaced the words with NaNs/missing values (X%)

silence/music/other languages in X% of the clip? [we decided to do nothing]

For each of these small, medium, and large datasets, we separated them into six subsets:
MaxSimple training subset (X words/hours)
ContraMean training subset 
K-means estimation subset
LLM train subset
Vocoder train subset
Clean val set (no training ever sees this subset!)


VAL SET EVALS 
Linear probing
All probing analyses use the TIMIT validation set. The two sentences that occur in the train set (SA1 and SA2) were excluded, leaving:

Number of words in the TIMIT val set: 11025
Number of unique words in the TIMIT val set: 2373
Number of unique speakers in the TIMIT val set: 168
Number of unique sentences in the TIMIT val set: 624

Common words (identifier: ‘word_val’): 
Samples the 10 most common words in TIMIT. 
['the', 'a', 'of', 'to', 'and', 'for', 'in', 'is', 'are', 'he']. Of these “he” was the word with the lowest number of occurrences (108) and to make the set balanced, each of these 10 common words were sampled 108 times (i.e. 1080 words in total). For a 80/20 split, this given 864 words in train, and 216 in test. 

Linear probing using TIMIT train/test

Common words (identifier: ‘common_words’).
Takes the top 10 most common words as identified through counts of both train and val data words. Samples the min number of words available for both train and test (separately). Such that it is balanced. 

Common words: ['the', 'to', 'a', 'in', 'and', 'that', 'of', 'she', 'an', 'all']
Number of words in the train set: 4550 which is 455 for each 10 common words
Number of words in the val set: 390 which is 39 for each 10 common words

TIMIT val set
We exclude sentences 1 and 2 (SA1 and SA2) because they appear in the train set. 

For chance level, we permuted the words associated with their audio embeddings. 

Phone and ‘full’: just uses everything

(obs this was before excluding SIL)

Logistic regression: solver lbfgs, penalty l2, multinomial multiclass. We have a set of coefs for each class.


Phonemes and too short time axis
Our model has 197, wav2vecbase has 249


Final probing settings
Target = word and cv_setting = full
Prior to exclusion of words that have same bounds or are after 5s: 
Train: 30132; val: 8128


Target = phone and cv_setting = full
Silence excluded.  
Prior to exclusion of phones that have same bounds or are after 5s: 
Train: 111297; val: 40516


Results

Game plan for figures / results

Fig 1: Illustration of the model [klemen start, greta] 

Bound alignment plot? 

Phone probing [last layer] [mean/max]
Word probing [last layer] [mean/max]
SI: confusion matrix

Phone/word QM: model’s layer performance + offset  [additional result]

Semantic similarity [last layer for comparison] [mean/max] [potentially analyze our model wrt layers later] [greta needs to add in our model + synthetic eval]

She-plot [klemen]
Roll-out (claim that we can produce diff things but all realistic-looking)

Cluster purity?

Scaling: validation loss plot? Task performance

Final thing LUM

WRITING
Greta: methods for probing + sim + intro
Klemen: methods for boundary discovery

Control models:
wav2vec-large
distilhubert

Sem: speech models + glove/gpt??


1. Establishing that initial segmentation bounds are sensible (MaxSimple)
Plots:
Visualization of MaxSimple bounds on cochleagrams
MAYBE with control: untrained model
Qualitative results 
Analyses:
MAYBE: What do the MaxSimple acoustic units correspond to (phone/word probing; within vs. between word embedding similarity)
Time align same sentence for different speakers and look at boundaries. Compute overlap? 

ALTERNATIVELY, we cut this section, and make it the first subpart of the next first section. In that case, we would have just a few MaxSimple visualizations as part of the first figure, but only as a “front-end” part. Similarly, we would not report any analyses of MaxSimple acoustic units. 

Bottom-line: our segmentation strategy is reasonable and fully learned


2. Investigating the acoustic units that emerge from [? whichever model we end up with]
Plots:
A diagram that shows all the component going into this model along with the losses

Plots/analyses:
(acknowledge that these tests are a bit odd because we didn’t design our model based on pre-defined notions of phones/words, but motivate it by saying that if our embedding space learns 
kk note: One overall thing we can do to make our numbers improved and more fair is to not provide GTWB. We can instead isolate a phoneme or work in an audio file, use MS to grab its bounds and CM to generate an embedding for it. We can compare with internal baselines but also with HuBERT or others, except with these baselines we just use all of the extracted tokens and try to decode from them. This will give us an edge and also provide a nice unification between CM and MS without saying “oh we have this thing that isolates acoustic units, but now we will ignore it and give ground truth bounds for this section”
Histogram(s) that show within vs. between word embedding similarity [greta: code up the exact tests that will span different word classes, frequencies(?), speakers(?), and number of words]
Controls: untrained model, model that is trained with GT word bounds (? “Gold MultiMax like in e.g. Algayres et al., 2023), ablated models with only negative or predictive loss (IF we go with MultiMax), HuBERT(?)
Linear probing of phones/words [? we don’t have phones implemented yet right? I think we could also ignore phones if this paper is becoming too full. Motivation is that we care about assumption-neutral units or at least words that we know perform symbolic computation while phones are more debated] [greta: code up the exact e.g., 1) using common words, regardless of speaker, balance the number of words in train/test such that there is a meaningful baseline, 2) do held-out speaker decoding(?)]
Controls: untrained model, model that is trained with GT word bounds (?), ablated models with only negative or predictive loss (IF we go with MultiMax), HuBERT(?)
kk note: HuBERT does cluster purity analysis with its tokens, this analysis pokes at a bunch of the things you mentioned above. We could formulate something along the lines of “this unit clusters more strongly with its corresponding word cluster than other clusters etc.”. 


sSIMI lexical semantics (per Nguyen et al., 2021). Based on similarity judgments of pairs of words [greta: implement if we want]
Controls: untrained model, model that is trained with GT word bounds (?), ablated models with only negative or predictive loss (IF we go with MultiMax), HuBERT(?)
ALTERNATIVE TO linear probing is ABX on phones/words/parts of speech (similar to Algayres et al., 2023, I think this yields nicer numbers and might make it more comparable to prior work…) [greta: implement if we actually want this – perhaps implement a similar version to the one in Nguyen et al., 2021]
Possible ABX addition: semantic relatedness ([greta: come up with GloVe co-occurrence metrics perhaps? – actually, rather do sSIMI]

Units per second vs X [klemen seems like you had a clear idea of this plot – can you please write it out if yes and/or let’s discuss? I guess we want something that can leverage/show off the plausibly low number of units for our model in a “non-fixed” GT word bound setting] → we’d make the claim that our assumption-neutral way of finding units works more efficiently compared to HuBERT, perhaps wav2vec or Whisper or something?
kk note:  yeah i think that any of the above metrics could be put on this plot, to contrast that HuBERT needs a lot more bits to represent an equivalent amount of information. I think that we should evaluate against HuBERT and see how things look
By merit of our downsampling… 

kk note:  I think that we could do something at sentence level here like ABX or even “bag of words analysis” on Crema-D, or any sort of sentence decoding. I know this bleeds a bit into the next section but basically if these embeddings function like words we should be able to 

MAYBE: Numbers on codebook usage

Word-level evals (based on GTWB)
Within vs. between word similarity
Word-level ABX (similar to Acoustic-phonetic ABX benchmark)
The ABX metric consists in computing, for a given contrast between two speech categories A and B, the probability that two sounds belonging to the same category are closer to one another than two sounds that belong to different categories.
Compute within and across speaker
POS tag 

Same word vs diff word (within-speaker and across-speaker)
POS tag vs diff POS tag (within-speaker and across-speaker)


sSIMI
Here, the task is to compute the similarity of the representations of pairs of words and compare it to human similarity judgements. 

Sentence-level evals





Bottom-line: our acoustic units are efficient (low number), not that many odd pre-baked assumptions, and can meaningfully distinguish between parts of speech, words [maybe speakers, semantics].

3. Our acoustic units (or LUM part of MultiMax) can be used for language modeling

Plots/analyses:
Compare losses between our LUM and vanilla LM on the same dataset [problem: how do we best compare the losses?]
We can run U2T on our model, but that does add one layer of uncertainty. Either way, we should do this to see the outputs
MAYBE: Crema-D dataset and decoding of emotion or Expresso dataset
MAYBE: ABX on sentences, but would need to decide what the “same category” means.
MAYBE: LUM loss on sWUGGY and sBLIMP 
but let’s see how far we get.. 


MaxQuant


Drafted stuff:
Reduction of information: how much bang for your buck do you get in acoustic versus compressed language data? Raw audio, leverage scaling properties.  
General: Lit search & thoughts


Goals:
1. Learn linguistic computations from acoustic speech properties via a discrete audio token space.
2. Hypothesis: A model that learns from “meaningful” speech tokens will be able to learn more efficiently (i.e., on less data) and capture meanings more accurately (e.g., in case of figurative language)
3. Build a low-fidelity interface that discriminates each single word with its acoustic properties (or speech to speech discrimination) 


More goal thoughts:
Goal is NOT to model linguistic learning? 
It’s kinda weird to obtain a discrete unit space just from segmentation or clustering. Seems like it is generated via some task: “Supervised ASR models implicitly address various sub-problems without needing to explicitly model each one of them, as demonstrated in recent end-to-end neural models”. 
Maybe only a short init phase of clusters make sense, but they should be adaptable? 
Some open qs
These speech LMs do not seem to encode syntax or semantics really. 
Everything is learned very stepwise 
Models do really poorly with in-the-wild speech (e.g., Lavechin 2023, cf. Nguyen 2023 expresso)
Scan people while they listen or perform the speech NLP tasks. We could then make claims about where in the hierarchy that the speech LMs “match” humans?
Coming up with a good contrastive embedding scheme?
Obtaining the best tokenizer? How do we evaluate how it is most human-like? Eval it on different levels? 
What kinds of experience are needed
Try to get at discriminative versus generative in the brain? If they’re separable? 
Let’s call it segmentation..
If we train max simple, we can train it on all kinds of auditory input and see what kinds of boundaries emerge. Perhaps speech boundaries emerge even if there is no speech





Related Work:

Textless NLP broadly
Blog post with several references from meta (https://ai.meta.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/)
Github repo from meta (https://github.com/facebookresearch/textlesslib)
Wu 2023: Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target: https://arxiv.org/pdf/2305.18096.pdf (20ms, vocab size of 500) 
Kharitonov 2021: Text-Free Prosody-Aware Generative Spoken Language Modeling https://arxiv.org/abs/2109.03264 (huBERT, 50 units per second) 
Gong 2023: Joint Audio and Speech Understanding
Nory 23: CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition Paper that does large scale contrastive audio-text pair pre-training. This could be a potential feature encoder for us to use during tokenization. We can also look at their evaluation methodology. Finally we could in theory work with them for some very large scale training if it came to it.
Nachmani 23: Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM Another recently released sound to language model but this one starts off with a pre trained LLM and explicitly alights the audio to the text and also outputs audionsoectrogemas and or text. So basically mostly a whisper system connected to a GPT connected to an audio synthesis system.


Quantization
Kudo 18: SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (https://arxiv.org/pdf/1808.06226.pdf) - the tokenizer used by LLaMA (a version of BPE)
Mentzer 23: Finite Scalar Quantization: VQ-VAE Made Simple (https://arxiv.org/pdf/2309.15505.pdf) - a new trick for quantizing continuous signals into discrete tokens that works better than traditional VQ-VAE (Vector Quantized Variational Autoencoders)
Caron 21: Unsupervised Learning of Visual Features by Contrastive Feature Assignment (https://arxiv.org/pdf/2006.09882.pdf)
Knorly et al 2023 https://laion.ai/blog/clara-release/ 


General notes on what other people have done
i) Speech-to-Units (S2U) encoders that automatically discover discrete representations or units which can be used to encode speech into "pseudo-text"; ii) Units-to-Units (U2U) models that are used for units modeling. This can take a form as Unit-Language- Model (uLM) for speech continuation (Lakhotia et al., 2021; Kharitonov et al., 2021a), sequenceto- sequence models for speech emotion conversion (Kreuk et al., 2021) or translation tasks (Lee et al., 2021a,b); iii) Units-to-Speech (U2S) models to reconstruct back the speech signals.
https://www.youtube.com/watch?v=lmSKFnckyqU

 can change the context before and after the token and evaluate ABX.
Can also pull the representations for e.g. couch and sofa and look at cosine dist. 

Textless NLP: speech inputs to speech outputs
baseline GSLM model, which has three components: an encoder that converts speech into discrete units that represent frequently recurring sounds in spoken language; an autoregressive, unit-based language model that’s trained to predict the next discrete unit based on what it’s seen before; and a decoder that converts units into speech. 
these models may help developmental psychologists and speech and language clinicians predict how infants’ ability to learn to speak and to understand speech is affected by variances in linguistic input available in different languages. 
We tested three state-of-the-art encoders: CPC, wav2vec 2.0, and HuBERT, followed by k-means clustering and deduplication (removing successive identical units). We used a standard causal Transformer for language modeling and Tacotron 2, a standard text-to-speech system, as our decoder. 
In comparing these different models, we couldn’t analyze the generated pseudo-text, because the units don’t map one-to-one with letters or phonemes. Good models typically use 100 units or more, and they generally encode stretches of speech that are shorter than phonemes. So we used a pretrained ASR to convert the generated audio back to text.
Evaluation:
Diversity of predicted tokens
First, it matters how many discrete units the quantizers use: A higher number yields better outcomes at the acoustic level, though at the cost of higher bit rates. Second, there’s a similar trend at the linguistic level, but in certain instances, using too many units becomes detrimental. Third, different encoders produced very different outcomes, with HuBERT providing the best overall result.
While the units our encoders discovered are not phonemes, they have many of the same properties: They encode phonetic contrasts (like differentiating between “pa” and “ba”) while ignoring speaker and channel information.
CLARA: 
I like their multilingual objective, ultimately, if one wants to learn a good speech space it's cool to do across languages (and the brain's speech regions are also insensitive to language)
A bit confused by why they say 100,000hrs of speech in the blog post, but 16,000 in the paper? is that multiplied with the augmentations or smth?
Some of their evaluations are kinda trivial but fine I guess (like gender classification)
They do text+audio which is not bio plausible when learning these things so I like our pure audio stuff much more :))



Arnon 2017: Words from spontaneous conversational speech can be recognized with human-like accuracy by an error-driven learning algorithm that discriminates between meanings straight from smart acoustic features, bypassing the phoneme as recognition unit
2 layer ANN. input: changes in acoustic frequency bands (Frequency Band Summaries FBS), output: unique words in the corpus 
boundaries calculated from the minima in the Hilbert amplitude envelope
20hrs of speech 
No phonemes: “Under optimal learning conditions, the initial stages of human processing of speech input may be far less complex than assumed by current state-of-the-art computational models in cognitive psychology”
They measure how well humans recognize the words 
Under optimal learning conditions, the initial
the congruency of model predictions and native speaker performance strongly suggests that the speech signal of isolated words extracted from free spontaneous speech is rarely sufficient for complete identification, not for humans and not for computational models of human language processing


Begus 2021: CiwGAN and fiwGAN: Encoding information in acoustic data to model
lexical learning with Generative Adversarial Networks
Two approaches for learning a discrete lexical unit space: ‘‘spoken term discovery’’ models and ‘‘models of word segmentation’’ (Lee et al., 2015, 390). 
Spoken term discovery: most commonly involve the clustering of similarities in acoustic data to establish a set of phonetic units from which lexical items are then established, again based on clustering. 
The word segmentation models, on the other hand, ‘‘start from unsegmented strings of symbols and attempt to identify subsequences corresponding to lexical items’’
The ideal cognitive model of lexical learning would include both the production and the perception aspect
GAN: generator and discriminator. 

Lee 2016: One of the most basic problems of language acquisition is accounting for how children learn the inventory of word forms from speech—phonological lexicon discovery

One of the most basic problems of language acquisition is accounting for how children learn the inventory of word forms from speech—phonological lexicon discovery [[great intro!!!]
One of the most basic problems of language acquisition is accounting for how children learn the inventory of word forms from speech—phonological lexicon discovery
In contrast, our model attempts to find a complete segmentation of the input data into units at multiple levels of abstraction (e.g., phonemes, syllables, words, etc.). 
our model integrates adaptor grammars with a noisy-channel model of phonetic variability and an acoustic model to discover hierarchical linguistic structures directly from acoustic signals.
many of the units discovered by our model are linguistically meaningful, although they do not always strictly correspond to words (i.e., the units may be morphemes or collocations, etc.). Since these are linguistically meaningful units which should be identified by an unsupervised lexical discovery model, it is not clear what advantage would be gained by privileging words in the evaluation.

Lai 2023: Audio-Visual Neural Syntax Acquisition
We present the Audio-Visual Neural Syntax Learner
(AV-NSL) that learns phrase structure by listening to audio and looking at images, without ever being exposed to text. By training on paired images and spoken captions, AV-NSL exhibits the capability to infer meaningful phrase structures that are comparable to those derived by naturally-supervised text parsers [[potentially good eval metrics for structure induction]]


Chung 2019 An Unsupervised Autoregressive Model for Speech Representation Learning
In this work, we propose an autoregressive model for learning speech representations that can be transferred to different tasks across different datasets. Our model is able to retain much information from the surface features, allowing a wide range of tasks across different datasets to benefit from the learned representations.
Following most of the neural LMs in the literature, we use an RNN [24] for modeling the temporal information within an acoustic sequence. Next token prediction based on the raw audio.
Not just one timestep ahead, but n timesteps 
Tasks: phone classification, speaker

Park & Glass 2007 Unsupervised pattern discovery in speech
Looks like a true classic [[really nice writing]]
Unlike speech, the lexicon of interesting subsequences is not known ahead of time, so these items must be discovered from the data directly [[kinda meaningful to separate into two phases, but changing the units should somewhat be able to go through updating. It’d be cool to add in text at some point and regularize the discrete acoustic space..]]

Kampner 2017 An embedded segmental K-means model for unsupervised segmentation and clustering of speech
this embedded segmental K-means model (ES-KMeans) represents arbitrary-length word segments as fixed-dimensional acoustic word embeddings.
Embed variable length audio signals into an embedding space and hypothesize that acoustically similar speech tokens should lie close together in that representational space. They initialize word boundaries randomly and then assign clusters?



Liu 2017 An empirical evaluation of zero resource acoustic unit discovery
Algorithm for AUD.
Evaluation: same/different and spoken document classification. 

Nguyen 2022 Are Discrete Units Necessary for Spoken Language Modeling?

Great motivation for why discrete bottlenecks may or may not be necessary
In this work, we analyse the importance of discretization in spoken language modeling. We employ a pre-trained acoustic model to obtain either continuous or discretized features from audio data. 
We then train BERT language models with a Masked Language Modeling (MLM) objective on both discrete and continuous features used either as inputs or as targets and evaluate the resulting systems on zero-shot spoken language modeling metrics 
We show that discretization disentangles linguistic information from non-linguistic signals, forcing the transformer to focus on linguistic ones.




Lavechin 2023 BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models
Our benchmark relies on zero-shot behavioral probing of LMs [2] and considers a spot-the-word task at the lexical level and a grammatical acceptability judgment task at the syntactic level (uses CHILDES to construct these).
Results are unequivocal: we observe a strong improvement on the lexical task for the model trained on audiobooks, while the same model trained on long-forms remains at chance level

Nguyen 2021 The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling
A suite of 4 black-box, zeroshot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics.
Typically, language models trained from text are evaluated using scores like perplexity. Unfortunately, this simple approach cannot be used here, since perplexity scores computed from learned discrete units vary according to granularity, making model comparison impossible
Lexicon: sWUGGY spot-the-word metrics. We built on Godais et al. (2017) which used the ‘spot-the-word’ task. In this task, networks are presented with a pair of items, an existing word and a matching nonword, and are evaluated on their capacity to attribute a higher probability to the existing word. The spot-the-word metric corresponds to the average accuracy of classifying the words and nonwords correctly across each pair.
Syntax: sBLIMP acceptability metrics. This part of the benchmark is adapted from BLIMP (Warstadt et al., 2019), a dataset of linguistic minimal sentence pairs of matched grammatical and ungrammatical sentences.
Lexical semantics: sSIMI similarity metrics. Here, the task is to compute the similarity of the representations of pairs of words and compare it to human similarity judgements. Based on previous work (Chung and Glass, 2018), we used a set of 13 existing semantic similarity and relatedness tests to construct our similarity benchmark


Algayres 2023 Generative Spoken Language Model based on continuous
word-sized audio tokens
The problem of using self-discovered units, however, is that these units are typically very small, in fact, usually smaller than phonemes (Lakhotia et al., 2021; Borsos et al., 2022). We think that increasing the size of the units will favourably impact the semantic capabilities of a downstream spoken LM.
Have continuous embeddings: We tackle this problem with a Lexical Embedder, a semi-learnable function that maps continuous tokens to a practically infinite list of embeddings.
They argue against clustering because i) hard to find the number of clusters threshold, and ii) Zipfian distribution: most word tokens are hapaxes (i.e. they have a word type that appears only once), and a small proportion of word types account for most Tokens. Clustering usually creates equal size clusters.
doesn’t quantize the tokens
fairly complicated procedure for training their autoregressive LLM
They try a non uniform binning and it apparently works worse than the 1 token per 200ms approach
They try SylSeg (syllable segmentation) and DP-Parse (word segmentation) 
And then they encode again and just pad or something? 
Model: It is composed of an encoder which segments the input speech into sequences of possibly varying size, and computes a fixed-sized Speech Sequence Embedding (SSE), which we call acoustic tokens (Section 3.1.1). These tokens are turned into lexical tokens through a learnable Lexical Embedder (Section 3.1.2), and fed into a causal Language Model.

To encode speech: they place a boundary every 200ms. Encode via wav2vec.
Then, they learn the function Lex Emb (stack of nonlinear connected layers; learning jointly with the LM). It has a quantization bottleneck. 


Algayres 2022 Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning
[[WE could come up with a scheme for learning contrastive embeddings based on what you were likely to THINK you heard ? Based on context or surprisal or something]]
we select our positive examples through a mix of time-stretching data augmentation [26] and k-Nearerst Neighbors search [27, 28].
They build a big KNN search and find positive pairs based on the embedding space. Not great

Hallap 2023 Evaluating context-invariance in unsupervised speech representations
However, one of the critical properties of phoneme transcriptions is context-invariance: the phonetic context of a speech sound can have massive influence on the way it is pronounced while text remains stable. This is why tokens of the same word have the same transcriptions—key to language understanding.
Current benchmarks do not measure context-stability. [[not sure I 100 agree. I think we can have diff representations for the same word in our mind too while we process lang]]

Ashihara 2023 SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?
SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines

Abdullah 2023 An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech
They look into the entropy over a given discrete unit and try to link it to the actual speech sounds. [[Could be interesting for something]]


Nguyen 2023 Expresso: A benchmark and analysis of discrete expressive speech resynthesis

The EXPRESSO dataset consists of 47 hours of expressive speech from 4 speakers of North American English. The dataset is divided into two main sections: an expressive reading section (37% of the corpus) where actors read short prompts in a parallel fashion in 7 different styles, with additional long-form and emphasis material (see 2.1), and an improvised dialog section (72% of the corpus) where pairs of actors are prompted to improvise a conversation in a fictive setting that illustrates one of
25 specified styles (see 2.2).
The HuBERT encoders trained on the larger and noisier corpus (Mix1) tend to have overall better results than when trained on LS960 only, especially when tested on Fisher.

Elkahky 2023 Do Coarser Units Benefit Cluster Prediction-Based Speech Pre-Training?
[[They do some coarse grain clusters but pretty hard to understand whats going on]]

Tomasello 2023 Stop: A dataset for spoken task oriented semantic parsing
Useful dataset

Gat 2023 Augmentation invariant discrete representation for generative spoken language modeling
speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated.
First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken
information (e.g., time-stretch). [[hmmm… not sure the premise here is legit??? Only if we assume there is a truly only semantic representation invariant of everything else?]]
But interesting manipulations: 


Algayes 2023: XLS-R fine-tuning on noisy word boundaries for unsupervised speech
segmentation into words
Taking inspiration from semi-supervised learning, we finetune an XLS-R model to predict word boundaries themselves produced by top-tier speech segmentation systems. 
SSL models are trained on large speech datasets to predict masked parts of the input speech signal. Such pre-training methods yield speech representations that can be quickly adapted through fine-tuning to a variety of downstream tasks ranging from ASR to speaker recognition, keyword spotting, intent classification and emotion recognition
Eval metric: F1 score on correctly discovered tokens. “A token is correctly discovered when both its boundaries correspond to the boundaries of a word in the time-aligned transcription” (Dunbar 2017)


Lakhotia 2021 Generative Spoken Language Modeling from Raw Audio
We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text)
The high level idea (see Figure 1) is that automatically discovered discrete units can be used to encode speech into "pseudo-text" (speech-tounit, S2u), which is used in turn to train a generative language model (unit-based language model, uLM) and to train a speech synthesizer (unit-tospeech, u2S). This enables learning an LM from scratch without text, and use it to generate speech conditionally or unconditionally, essentially replicating what toddlers achieve before learning to read.
Speech-to-unit models (can use k-means on these representations)
audioMAE: 
He 2022: mel spectrogram
CPC
For example, in Contrastive Predictive Coding (CPC) [14], a popular self-supervised method, the auxiliary task in next frame prediction. A CNN learns representations from the raw waveform, which are then fed into a recurrent neural network to generate contextual representations. The model is trained via noise contrastive estimation to correctly identify the correct next frame from a set of random frames given the contextual features.
Self-supervised techniques such as CPC have drastically improved the quality of the representation learned from unlabeled data. CPC extracts a feature vector every 10 ms
Regularized CPC:
Wav2vec
hBERT
(with different priors baked in)
And also! Clustering approaches should re cover how different words are used in different contexts!!!!! Could find a good clustering solution on held out speakers or smth
So we’d take the embeddings and encode a bunch of speech, and set a number of discrete units to be X. or using a VAE.

Hassid 2023 Textually Pretrained Speech Language Models (TWIST)
1. Speech tokenizer (raw speech into discrete repr, here we can use e.g. MAE)
SpeechLMs are trained on extracted discrete speech tokens (z). Such a modeling framework additionally allows for capturing and modeling prosodic features [Kharitonov et al., 2021],

Kreuk 2022 Audio Language Modeling using Perceptually-Guided Discrete Representations
Perceptually guided: could be evaluated via human performance on masked audio and which slices they recover! Which means that it goes together in a unit 
Causal LM on discrete representations. 
three main components: i) audio encoder; ii) audio decoder; and a iii) language model
Next, we use the discrete representations obtained from this model for learning a language model. Then, we perform conditional audio generation by priming the language model on a segment of audio, and generating a suitable continuation.
The transformer decoder model is trained as a causal language model over the sequences of discrete units
Can train on different unit granularity!!!!
Gat 2023 Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling
three main modules: (i) Speech-to-unit, (ii) Unit language model, and (iii) Unit-to-speech, where each of these modules is trained separately. Speech resynthesis can be achieved while ignoring the language model and directly feeding the quantized units into the unit-to-speech module (Polyak et al., 2021)
So in principle the units matter more. I don't care about the unit-to-speech. But we should be able to transcribe the units and see what they are! 


Our models / approaches

Evaluation metrics:

Comparison model: standard GPT2-type of model, Size: ~100M, Match number of words with the text corpus of interest

1. Standard next-token prediction (this model should be able to learn to predict the next word). Needs to be transcribed.
2. Entailment (NLU benchmarks that have neutral, true.. following sentences; predicting the next sentence)
3. Figurative benchmarks
1. Irony, sarcasm, metaphors, affection
2. Construction of benchmarks ourselves?
Creating a speech-based version of the text based stuff and then actually compare
Qualitative evaluation of text completion
mTurk 
Efficiency of training (only meaningful in the case where we have a benchmark that compares between text and audio token)

Other things to quantify 
Tokenization per character (i.e., how many tokens does a given sentence need compared to BPE?)


Eval metrics (oldish notes)
ABX: The ABX task examines the discriminative phonetic abilities of the representation. show that the ABX result is a good proxy to signal content (i.e., Phoneme Error Rate)
it consists in estimating the probability that two tokens of the same category A (x and a) are closer to one another than a token of A (x) and of B (b).
To evaluate these representations, we take the view that, while representations may not correspond one-to-one to linguistically interpretable units (phonemes, phonetic features, syllables, etc.), and may not even be discrete, they should at least support the same key function: phonemic contrast. Phonemes are defined as the smallest element of speech that make a difference in meaning between words (e.g., /bit/ versus /but/). We require representations to distinguish pairs of phonemes, while ignoring non-linguistic variations. Discriminability is computed by running an ABX discrimination test ( Citation: Schatz, 2016 ) .
Why perplexity is bad: Text-based models do not have this problem, since the input is already expressed in terms of mid-level discrete units (characters or words), and are typically evaluated with unsupervised metrics close to the learning objectives like perplexity or log likelihood. Here, such an approach is not directly applicable even if we rely on discrete pseudo-text units, since such metrics would depend in an unknown fashion on their granularity (number, duration and distribution), making the comparison of models that use different units infeasible [but we can still do it within-model!]
Lakhotia et al. [2021] also proposed computing the PPL over the transcription of the generated speech to assess both quality and diversity of the generations. While this metric is automatic and easy to compute, it is highly influenced by numerous factors such as speech vocoder quality, Automatic Speech Recognition (ASR) transcription errors, sampling, diversity of the generated output, etc.
generative spoken language models can be evaluated at two levels, acoustic and language-level:
1. Acoustic unit discovery (encoding at the acoustic level) consists in representing speech in terms of discrete units discarding non-linguistic factors like speaker and noise.
Spoken Language Modeling (encoding at the language level) consists in learning the probabilities of language patterns
Per Lakhotia 2021: spot-the-word and syntax (transcribed?)
Speech Generation (generation for language modeling) consists in generating novel and natural speech (conditioned on some prompt or not).
Speech token perplexity (PPL): not comparable across tokenizers, but helpful within model.



Details:
Tokenization should happen beforehand. Tokenize entire dataset with BPE.
Compression is the critical thing. How do we get rid of what we should get rid of? 
Different tokens in audio versus text subsets? Compare distributions? Train models on subset of openwebtext and speech data transcribed.
Getting the discrete units: 
Very naive: do some kind of clustering on phoneme/syllables without audio and a window size of X.
Compress spectrogram?
Extract spectrotemporal features (~30K) → learn a discriminatory representation from speech (on short time scales!)
Edges in embedding space
Contrastive Predictive Coding
Other audio encoders such as wav2vec
AudioMAE
Whisper encoder 
Different layers
Image quantization model based on the spectrogram


Ideas for discriminatory learning on audio tokens
simCSE inspired: pass the same sentence to the pre-trained encoder twice: by applying the standard dropout twice, we can obtain two different embeddings as “positive pairs”. Then we take other sentences in the same mini-batch as “negatives”, and the model predicts the positive one among the negatives. 
Or come up with some way of learning that the same word from 





TODOs:
Create 1 billion token subset of OpenWebText
Train gpt-2 sized model on 1 billion OpenWebText tokens
Train gpt model on speech transcripts, still BPE
Test gpt2-openwebtext_10perc_1024context on context windows of size 64
Create an independent validation set which is distinct from both speech transcript and webtext distribution
Concatenate or take youtube subset into 30s chunks and feed into whisper. 
Get hidden states for an encoder layer, and do dumb averaging of the time. Then perform k-means clustering. Then we get e.g. 500 “prototype” signatures
And then we can take new audio and feed it through the procedure and see what its closest to..


A few notes on whisper 


Output after feature_extractor (whisper encoder) is always [80,3000]
If 2s clip, we have 80 features, and time is meaningful until 200 (so 201 vals; then it has the exact same value) 


Random code notes GT
Training a normal GPT model
First tmux the cluster (tmux new -s g tmux a -t g)

srun  -t 05:00:00 --gres=gpu:a100:1 --mem=64G -p evlab --cpus-per-task=16 --pty bash
srun  -t 12:00:00 --gres=gpu:a100:1 --mem=64G -p evlab --cpus-per-task=16 --pty bash

srun  -t 05:00:00 --gres=gpu:a100:1 --mem=128G --cpus-per-task=32 --pty bash


python train.py config/train_gpt2_openwebtext_small_context64_iters60000.py

Training a maxsimple model




Quantization values for cochleagrams




Downloading TIMIT
https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3 
https://github.com/philipperemy/timit?tab=readme-ov-file 

Running stuff from om
First I ran from default='/nese/mit/group/evlab/u/gretatu/DATASETS/TIMIT/archive/data/TRAIN/')

Moved to sftp://gretatu:@evnode.mit.edu/%2Frdma/vast-rdma/vast/evlab/gretatu/datasets/TIMIT/ 



Some words lost bc if row.start_time > target_duration

Investigating unique words in train/test for linear probing for TIMIT
Loaded from TIMIT, variable: chosen_words_train/val:
30132/8128

Unique of these are:



In variables embs_and_labels_train/test we have the loaded audio, i.e. after removing the words that are outside 5s etc. 


Checking the unique words of embs_and_labels_train/test:

Filter has the test words where we remove a word in test if it did not occur in train (4 words removed)

We put all labels into the cm matrices, which are from both test (GT!) and pred

So because the probe was trained on a larger set of unique words, it might think that some of those words occur in test (y_pred). So this set of labels can be larger than the unique number of words in y_test:



https://github.com/gretatuckute/naudioGPT/commit/9bc4edbf4df16d96ab1e8e3415b508fe3ec8ae2d 
Makes a lot of sense for small model.

Did it for base, also got at cm of 1153

Klemens run (on 0622):  looks good (for layer 0)
So for layer 0, we only have n unique words in test as the cm (meaning that words from train do not get predicted – we largely predict the mean?), but when we go deeper in the model, we might expect to see words from train, not in test, in the cm (making cm larger): https://github.com/gretatuckute/naudioGPT/commit/68aaa7a334d062e5eb8461d4322424db17088e45 


Spectemp notes: 



ENV_SR and sr_Hz seems to be the same param, ENV_SR not used 


Seems to be the same as env_sr



